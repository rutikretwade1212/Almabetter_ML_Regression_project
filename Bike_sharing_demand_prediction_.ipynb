{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![public-city-bicycle-sharing-business-260nw-1616400925.webp](data:image/webp;base64,UklGRuJYAABXRUJQVlA4INZYAADwFQGdASoIAhgBPjEYiUOiIaETO6z4IAMEpu8Wl25k9SBKQJeze/+3mO/0naVje8P+SX9Y/63+t+Z/jnsT8XffPzr/bf/P/n/vx/l943zP/D8tbyv9N/yn+B/xf/I/y////+X3Q/4P+09mH5t/2vuDfqZ/lv7z/of+b/ff/////wh/xP9D7Qf7n/xf9t+t3wF/nf9k/2/9w/er5df9j/wf8/7mf6j/g/+N/h/8t8gH80/sP+9/OH5xP+d7Ff+D/4XsB/zL++/8789fl2/3n/u/3n76/Rz/T/9V/5P9R/tf//9B/83/uP/a/P3/h/QB/4vUA/53//9gD9//b37F/4z8d/el82/iP8N+Uvnv+R/RP3D+9f4//Ef2L/x/7r5VM1fZJ/kehP8r+vv3X+/ftP/d/3b+X/+j+Xf9y9L/mP/X/cN8h34//Jv7X/dP2n/tX7Vet73Vm3/8f0FPYD6R/hf8F+2P9//dD6EfqP9t/k/Xz6//7D8sf8J9gH8o/n3+M/uf7ef4D///+T78/4XjY/df9t+yXwCfyr+l/7f+7f5v/xf576bf5//mf5b/Pf+D/U///4K/ov99/5n+J/0v/t/0n///Aj+Uf0n/P/3b/L/9T/H////w/dn/9fcF+4P/i91T9dv+T+fBSUCYkU/bl66RPuBE1FsSKgyYkVBkxIqDJS+0D1/G8M8TNxYVH9S0qNPFD4FPenB1MnPWg3nUg36zD33YAACQe3gGOlaac8kL9tNc5PoMbk+12ssOmT6B5Ai/OMcMiwqt+y9jwo6c2EwBfOt+rlirm+msKr43onvuehsy924/G+/7LZ9rLVIslNuAgx0mbs8YuEqLYdku+C3h8U0RD2hPf3lX+POPa8FTlZiRH5w2ckBLn04galyII211WF0ht7PF8OLgeF3MQvrdFsWOJS1mzE/d99uuJfvIQS6hqoG29k+lQdKMK3RWFKb/ySPijYzKGFYQN7y0KjT3hXHj5I2ftzLdrhJzb2l0Be7gB1C2iFbkB4C9rKcbqyuSaRCQnATnlX0R81ACh6tc0mn+L2sP1R8BzjOHFj8PSTGFP6EfoxsWTGJwMXzxij0najPKlqRmyeANfAYqvAK7uoEzQqJfMC6bqgIqDK5JTrt+1uVpz72DENvjdUDT+SoZs3iZt9lhKbH6q59/YFfBkX7oshXYuQqhRfuWuGuWu4U1Ici6RV+et2GWfZ+HmvIexpYmMm2R/CNiaS4bGtUVJy/FT6VDZQgWuLjXtq5Q9jp2zFy1P4KObTsgxiIy46rgNxiwLMPkhQuG1MHJrDxkmwtQ5wqRHNGahOXi7pBlSdy97UarSEqtEmvcFGdwPgN96PVjoXGGpOHBrxKRxaMrAcrh9vcCCq17QyV5ytIBqkMFDIrnhnc/kVFbK6EpabqqHmwiYPPSJWPoHrZpxfZ9kKMImjDNRdyRh7wcWqlPKKfiHD4qiUz7bnGvcz1F3fXnsFuSMOD939cr9XskxhFj2SocheDPxJhVE2+HkXMlGY6MmlN6bLQxaoFsPGNtn7r1NHAWuZkQHpaoN6ppa/6KS+45+O6j5fMQpOY051+ftTmijr44FAv52q/4DxRCl0ZJGN9lfZ+m27nOhZO1urNrWaLQzoVil+R9KiQtaLqVHbLBF14u/IyGRXcdK9jYmlR42HBbqqVY7LD6eFmwzaIxPIbxmdVhrJI6C23KTUn8FzH20NgnBdUfqKfh19w8vKx3d3gE/hp2sbylg7ovwe8MVvrtvdWd1ln6RvCM/R2G5J/G7PsyU4hUae8MmrHTdETSHS/rrnks2ssKcqwvBLnmV12U4QIbnERT9VHKCdAJjRee0x4w/JRzyRXqGeZp4/jxreU2PM5k/IuTrYgtq+Lw6jUuCyVToqgHaGkc5zL6g75C2rGaoLfJex2CNkwX2uC4Fg59UK7ZZEoBXM/d/CIBcM0pmhgJPHhYLTOMLTOMLUFc66mvqX/LQECnsQ3jjsw+gqC4CGf+s2OU4xqg5Wc/+AkjWm4HdvIFdeYXpB8pCFKj2gUwAjx2CwJJwWcyZ2VU6/ENkodYGv5oAy25C61I2n8AbLymzooSAFL0pGHJPHEAicRMz4z0GKik3VVat4xV6oeSniQ/56Pqi7CC8F9BRt4nOBpzmLx9HH+sieY3SM1gcBsm8iX+P2nGcvILUY4b2iCMsJYdcDYxzRfXJox1YDhMjGBbBPWtju5fwTXlqCu1N9bo98v3bUEwkodL1t8a5htITosh4JrGDgKGKl18SvhSIALv4FbN9cG4/FBl3Ysx0kT7M7TaMH8l4w7y5IflnWASSkGwSLtn2+kFZpjeXyCtYHzg+3rvf6yKsIg4OkcSueSslO9s2ET5LIlG3BXJnl5w74i11/bHapVx0eIVIIFP7ndj0b4fsBXZPjNq8Mg+VyZ4PYiyBv+RP2DrX2SCq0bwb5K18w9t4y5AmhE+SgE9XKVUy0Y2naCtg1NX/qmSnKYy7xHrshBE1FpP9s+XrYNR7hsX9fpG+rV4NUUZGR3CoNdwvQYz5C48hC3ogr8I47f4jzJWhlG73TG8m34Bo/Aqix1utgxiw6a8psoWQb7JcNwLUlwn+SfphnD385Jbt3jfsimH9TPXix+TCYJbsGN+dZANfIbBWBzEX7KP8ULOl/0NNC/HnVIoJt4VPJsI43EioBq0v5n60XcPTodsbHxkx/4FHqer3ZjWpRJmAj0beqFj7E1DSUtd+gg+fWT/iY/urERkV67mtVwTNgrgFnTc7wjrECSnzWvOL/YtWiUpG261zqztjwJSfSHPbmpU7sZpMSKgyYku56o08dVeuZ5cSIWeKHwKi2ISXctJ3V4I/IkuHdbps0RVpUzm0gjZZiOufuPt2pPTizaonJM9TsZHA29SQHWqhLZVjKb1jbBpnUrJ7AdkS5IeLYkVBg74UaTbLk8nSMIjVB+OWwxf3zyLSplEPO69K7FHsBFbXd6DjcSKUAAA/v+r32RxBSB7h9YvrjdcHbUAT7sJrssB1EAAAG6nkBmA5qXrkt05inyGAAGIZF5SDexJLJbMGwUFahYQvwXQ1NFcSOcczWVJ/JdOna6ftbeHW+0PgACJvHFVfaDhaA23yYSlhll2xxCpVD94w5olhUMyLli5VuwtwLHK1v2GsRSABgmuOvrUubvl3HbnxNAl0UgLwlPB/ljZN9i8x1OVj4Xh7BbrgElwyJKvWaIfvXwao9me283xNiC6RZskEvOFxRN5LyA713WZ1xdeAj7nM20uAK2U6HoU265MAn2UQ9jqArwuEKOzgxpveizrFSrGEismQhb2jna0mc6H78dL18VqUWtf8hUSQcjTCpn55c60635v+1wd/w03CR6Sq5lnB3J+eYZ/YR1lCkl4uVxojDztOZxMMWnuxyIseI7vKkicGhitYiPG+esCtJsOpE7wzdGIqNsJh8+wL9kb3ELPAIJvl52Yu3TvXIVr3wdDEf2KSE/wbTtOuwQEiP4SaZJHdbpJAuRnPMIkaLQ6jgbkNY77Hu58k64y6RMhQ2rt3lRKfRJfBRT7bHsDnU0/CbcZM0sBxNQ/tfYTSjC/m9QVpPO4lqpG6kK9kg28R4nNlepTbfJLa7FGSfTwKoBy808/GSLkViuGcIm6VFMrFIhQ9uzD9C16mV7CsQZJ8JvUKom6QxNYOCUn+ntqiqJOiP8tkPkxYECEGQBQ79RECTz+V4D5z73Z3Jk+HMLQKVEE+4mhmPGUBcpAOn1Q4NNO0kGs7YcavdnSk2p2yYI/AwD8kdDUe0bMB2h+HF6qYMmVHmxbfzwhTQX5NhGAg8xgiceuDj1LIoCprjh6yhlJx2g1XQrtrzXTVnoXxyx+2+JqVQ0l3mhd4mS1wDI38TO4inB6HeYWf41FlLZ5IhNQzP+hlrXn1L7UKWt75vdmz1ChAylb0X+NruOiCPqNGGKbDPkIlgIovGTccIxqc6uehrnMZQJjfOjCa9cH/eK+GMfyFHyi+xdvwkD3/U+UUTycuxPfzwgJG9/2EbnK2qab4lVEn/mkmY6JHK0CDFU0+dxs2S5BhEbkk4gXwBowmgQQnDmn3AlMtGbHaifOyeRM980O7APKOUB2kpqXeqHgxP+weW1aZiWOPiZmHuIy6/VdtdTiSiIPVauMz5jF08uNBwSJdVnlTUuBdCl6ITX5jtNO9LZvEyo9LKMKZFihp9TKu95LicyGFhcJmyYPn7oVIlZrnNw78abe62OSBJyKOMkUBDxaJTIienc7E5ORC7Zcq7rt+ETOIhkJ/3u62RYLSod9XJOULCMb8Wy1gp+WOFUJ9ZDjuK9KTPgsfiSmqB+H8+afNLYpou6da92enDnQsTBJY7nSmWpUCqWTNydklTSu06aotNMvhKATVSTgdsUpwArkMqv2FzirFY0xiB8/YFJFIxJnEm6mynIJwqUQz6LGvp1enuQ0dJExCd5TjwMaF+J1+1yVlcEgg6KwhUc2k4Lgna5Li8vz03KW1bbYrTwAEGRiCpMCvePWtJhigpIJ59XfaGYLSsXCJnzk4NYUeFJ/nrFkbESWErahBM8FVk5sLtYILbIUPxlPiJ/id550wgkrDerR1fcxQ5iLgJgEKhUjklVlUqGuK+nJOqsmjWSpju7e5IaO9sgH4GLl2bMXBwQGmvgF+XW4F6TX84KHeVARdclLtEI6KozaIqAGUdXPANz8RD5uCCFSJVSJ0dZ5QYLTCUrA01R7op8LKmBnRyWHS3bzk1pRWHv/RWMl6g7ayNbgcvEzn0L3bbjVNBMpXq5qeOPKFqIk22pInYv78g+/bT8R3YTgT4h9mpdQVOUYj7NS+5U/38SZog68qqh9N51v9Ul7p7d/s5xC3G3ciWaW6EMfNhl7ww/hdJJ1xZ+s7cTfK57fMCEVNnGHDvERBcjFOkR6LLmGXLBGUuq1DJCuct/l5vc7vTO1SF7qZEquSWsLzVIwd170Pu1B8czQbLdhJF/jFXNCtL9nNi7DcNgUN+c02Pl5YCaOkbRehcyKej86i9Vqw5j7ffewMzXa6NWer2nKcFQYORWddrPtgkGu4o0zjEDkLQFsuwGeiGgLb36sojD3h9NcIALJqQtRDBxxxyAmLkt45sjyZL4BIjb+m2g/mCqU51GtdhZ27gV7ho0MNp4av29YCM4JniPXlb8f3MLwZ4YANdyLvRDHP8vfHSJfS24a3EQCZvrJ2uq0JzuDrpKjG/qqv13NWDkCE4WQgZ1crafsuIELYXBU4CWwMX6fD+j4fY0SgvdCmqaDtcznKeP9NQt1ujwb/yyDVrwhJ9YmwMmFZBrKRl4pcX/ZrT9XT8rvvDj4m+/djvDYHgz/vnleehfjozu9M3ulZVIohNSYFvHoRU7sQqsAWp/FQJ6os3PisS+1nk/PKWzyldID04DH4ghcfZi/RuaqSEE5MzWa5nZuTsTltX67GacdYNUjGie9CDeBTQXSaKSef5nMb5hvDtp+gkM+blvbFdGpUDK2jFSqihZDn4oNt7Pkv4DaiIXhXHtJfK7ka/lqIGt6lyMTgmnn4ApOMd0nDhAbFC4Ju6sSr4X06Mfn1b2mdX1C4eF6RxLhtd4COBJp/CKWLM0tZQYV+hHhb6RdzTFd1gzbQa7dCXa2tMuFN5zQO+HBYEKfCMmwCIAJTxH6CT5AAwM3kxs4/VmvAVxqxYCwNZtvORTjlgHhLgtm+admoDWxU84JBSW4OmIMGTkRBvdYQae3ziLrffi7EH0dhbYtItCwICMAwwknpTZvOPPUb2utsCytQ87jsKcb0sWN8Fu3YfVREOg6xWA43jsLVgCkjVQrxjPQdVn41p+58gkLNrSnTEkYI4UIpLeJckFlgNY8SLmS5oYEoyp1hBJPTPXdov6AuIx11LCUaClnNzZkKh0xYrFUEv63bA44qqqm71svUXrG2k+fze3/wuP+D44rAwQ9B0dOCY+b0hM1X7FHEqh/DbmAMdSZjI8QuT2Nsz81IG5cY7uZ8rKWAxeNMQFcYW6Q/l9yPTEFVt1DZZefvq17rivBCsFATD4M+8ZkSJYPelvEK+Odhu/6oP4wZbXkPRGEh7NvWijK3SDcXHguhZiyh6qOv7pzz38I4da8omZtWPF+ddSq4l30uFSYtccaP1Wh16jtmcLyRD/X4nGcDp4j6M2o+AslfWrn/+JV+15fG9qKjTAJDSksIibGN7GMnqjpzpZrL5jWzPWO9zcnkDWi+LG58KGz4ft3fRPMgiKUhq7esF3/tIl8sCOxOQ68BtCnezVgqXnRh8iA59O+B66+KOk5IjD2DiV0L4iu3pQaSYOrHKeGSe03bJ6rht8qZwCe3c+7YZhMklfFrn+Lomgvn3Rz0qTJsGjGHJ4KaSXyJ3Vz1CwfdKsU9xMkZKnNcu5meCQms576t/wz8j3pjaKEVWUmph8MqGFjCAOjSNtnIopT8VVpNeBLG/SVqrVFX18YsiU5M01fnGKcdx/vvwrXZlBG/mRbiMCeAqS7aJEdflKaa5I3M+C2kFLUaWxVBSLRQkArlsTh2fsywm8bILYJ3hbjJrir+e0WpjGizlDRGCt8VNy8KzoXkdnaLD91txAnMXG+5JBl2WLtSbU9lH3ST7javuibG968KTVPXN8H6JSnFnfZhrf/iV/Kfz8W37PCMw7mICSoISINQofYg/LbnCnLoljx4XvWdlWvDAkg0oAhqVuZk8DOXma7wjah+xWF2ofBqp8es2++pUYMx26oWICt3wbUV83KnV8tipDUYwMjZniUGYknHJ16Pr7rDV4PvSROa7alTJdPetIr/gjogtmaZwhyh+SOXnDHtbFlrC94+RsCJc/sbwr9ok4RNlWzyerSPNjarROmidMvC21EJ/jWkW64zPf3gqUPLpm7/z5Addct/K2T8CWtRonNgG5QmvbDTEpD8ylrKT4GtyVN5FeaazTafy4j/O8l6PT6GcQfVrPxzGysUJKqLeCQ5WwZoz7LcWdD04emOBbuGfSbElE0JMk0QDX8mkXWn8/rMR4CKYueGo3ZoEo4o0NPxprTs3P48xhQ3uHvsrABzCmDnxLOlRd6C5G2ZYwAhrThSe6RcY68WcdU2UoY4mRzXViFsJNgBFlJi0ZMqvDRV6jSR6gTiUVCey283bk6eOAqqNqc47mzYt9hVkZA3NZMClzq3jjLgMXzVEmnHLw538DGdKGxJ/wo9GNLREWEeNcxJDQNRLH7DMvt5kRToLS2nQgRJ87c7kYnIJkbQS5ply/ILdBxz6uGRPfkHaKq4RNtOJyqjFPdDfQxYmA1TCqt3S7+uaPu5caDe5t6SpyGhQuV6AuKdNop00d1ba5a69EtuvS9R6pDjh6ALIIZau2IERIHP7gklgvqs6xQ+LVjjM+8Pb2x2i/lm6WitNq7oVUsPMWYcotJYh2G4lYHRk5a+yYp1K2yWKhm6G1Jn7mXwFrbeJzDHbFTI1+q2DX/BWJryHXUJjBX6YpZXgSvkypd4SjJZ9np9J2gsut5+xc1134cAwXFJklzAiXR1kMfzmH1asQB5UjujmGUD9SJNfto6dECb6OXr/2ePKS+4ogihe+PP3BX188RcTqVviDkUb/2NbcS3SCzIvmJfNKEOSCKALO9/geAd7UkgAc+Vwlu8vF5u/8Z/rxWUeW27HI7nkOpg2lqu5hHf7WAxiejVBLwYxvPtqrkzq7XHFpuDOkd1Xb8uzxrpOGhZ0STQ2OfYb45lPVi6gfaWpJ/Zq3SxbaSkrIIyBVMRBdMG4IVnC2X8dgmbdrK+ixIlBBkl6fUdDIh+1lzc1870hrZn/DMIvUTfOcmm2/1Z9bBrIQ2kBGV7g9kHnbDoqeHy5quWFXmq5X2lISAx+RNspYZWrTPJw/3g83xjtnu3f0tlkLJR9zXxiDNhifo0qpEc4yKnbgHu5n5PO5KPePUmAWmzMg0SzzazZEnE+VgsWSGRGMLxxWX+M/J3i6qVaMbI0lVuDbxTDL2cjdxxAqx2wSnqu7EWIMLU3HNlX+eoa+gGwLJYVKPsAMzG+Lk12kMiWImhZk3kjeSUEwAcGrThg32IbIjBULM+ObODbS6xPPaYC/FikehgMgrl6qWi7nht9uJlCiuc/4fVOGRYebUDb8X75bw+7OGtmr14VPKLeMV2PClshlRDhxIP9aDFkpvGQxof6POAeQh9wXYwtYshmnyFOJbZxbPIBHl/JQWOYwtG7YYyzpbpUVyqcyIYU5OQJHWR1FuZdCyrA9IhuX+fxySdRtvDnWMqqrAh9q1ulOtrfxtOKZIEOcNczXDprlGVLM81SJZM7DW0O2TTlGhAh+CGXREm3C/QFYFPIeBGin4myN9yfe/7jc+ZmY/AgwKOZibMoSMVNDBir4laPQGM79xkVQM9aQrtf9l5ngPGwmzlEbuoS0/vML1SgSuirrwH8k9FDiF8Cbb+14cJ/amJgZurUlPcftGpXgTcgHwrGKNMANeSlbIhSTvEpjft6RzU/5lxHAHrpwTMrMf/bdlzXZl1tR4pmQgfCdC8sph36FeQLYQlE5Q9AuQZYUm7ywn99NXiJ68vFMr5U7FBdRHE/1QWO35vfLzuG0fxOSvPxFLO1GsN4OZtSRozKI+fb+eYbH0uN5Z0fsJ/SHO8hUu6VWv8PPbpcMtzFkKL/fQjhwLD0DSbZ2hUWVEWyiRJE8biHeP9UCSCoLsaL+pBSVAJWi9ARkAYKgBqxXp25/dFqOhHr4geMifJ+bVI6sJnE2UTvOOW5+2mqYlAHoU/+5SY/xHIn0giwA59vl3B/kQdohtJI1o+UcN27mXz13gPMArE/b0P84KvXWqMz4bb0/+oWt3TxnccxmgtGrrdxYFmlhhhmmkM0dzH5Cz6UbWQ4XLDW17YxKZTG1WHoEwYvQqJxS2Y68XozLOV9RZxi4T+kvqOiKEkRphj2rdj/6ECOGFRD+C2m1fcYZqQFSvBRuo3xZof4uXhcNXMmEpoGEnmGpTRJmmZE7Ko/1458IUxtu9f7gb7X7drzdutStGn6Lnrg6JELM6zaQGjtrlCNDrLePDZu6g9r+UzF0h+c/iW3m0RZx2yEOJLVNqQ7nTcPAt+oNdCVO5x6+YS5npzwWmBDBKZXOY+DAB8hs45U9QqFyKXZB2fJSL7txpu7R4U8mxRkWFUJX+NJVbatryawkOqF6sSSYde2UqUJ2EA82pacvMkevHjtwsjeZRsbYXEEWVw7FbUoB9UWsu3lnaaRjJCg20kfXcrO/OqIOlGbxgbPLssQ84sQPJcbosy9alUrKdrS615YOOVss0QvIG0disg6tMCOcBLgKRBHBCuSvD5UVl7a6kx89N/TSG3l3Bt8f7tHc247tqCawaXeaMD6XhUWCzjBoIqTVutFWuWdqAYIwck688IfNjLc1uwz25sG/177+shMUYT6oF+lpBnYzJ39alcks7C/vFwvJ15L5Ue2s2jvFoptnmMVL4vM9L0T1eGiuPgRYtnV6gpn5UiORY6frv8YDehPylNC+LswKYcvZHCjRIHhCGDum1EskqvK5Gog3REtqWI+OTs8UCdMiFHfYCwebKNSuXv32WE6yobN6pftD6cOUC7N7TyliWA81pzThg5aAr/LU1kIehYNp8qHAl97SMJjTrEaIPm0Vrk1eq8VTDoIvBixBSq3/jQIKN7vesB33nVyzECwPBPzamhNzezaKp6LUFpElRNIvZMYZqL48ZaVeABsLJt36CRUBV3e5yATZR7ND7G3M3IWiL2uc2IrgsoPq/GbJyLLtVnCsfOhZrOeenjVOw/8qDpV4VBsWWSNsgBPv6N9wyaRixzH+CxzOfv96Qxb0kWNO2CE9RVpmPX6we4GLAsnigiE+OOpeWQoegpTap462d7mTzgi4G6r5E2az9VM7yb0v5Nh9Vw96faahH+nm0FjtRabnQX/KPjSCSgwvJPndue2Jh6jb7SU4Y8VA1iw1M1j8ehKmehA1p5s9xBLkGAQkK4EelzVhYo+mWzezujWyrGvsrwaTzVz5m7bfHmgH/sKpNp8yP8DjdHm+7fa5HqtOZUkPxp6RyqxtqjHERuJDL2H6+dKp0zm6suYw1MX67g87x+49LNoa294B8oLbxOjZ45zO9gtDvoxwkWHEy4rl0JBq5IH/6iB3xLBJdKbLAxKneo1G+n+EZIrf85hxoq5/lXqEIOaIq1WafQvOyczOH6GGmvJ8/HgX3XMdTX4bKyQEw5Mafeu0wFrJ4myIO0l8d33aMYEmzyf1ZFfJIDIWjNewnCKyrSY2LVdglEO8ZK/Q+wHmax/NjqCbxDiFHgOt4tM0nxng2PlC4jganbOjJk9bp451Gqpeq0f0uRVdnNlQ7iSBKgjezx23vSZFDGBkAWWJ+7rqDFgKwdl3cArSGB00Rfq3tYNdi0hOqfHkCADgFmWSs4cPLISwvfnUTU+ofggzFKs2+2tpCeO3x2MJ08nooDNjmu4XI69M/+x+HI9JzDVXfWpY+mwcRBe6FzMRXUUPgqVuq0zqjBzl0qMx428LrURqL8zWB8pPYgdy9m1zc7r2Rn+Q10RFqOGy9wl+X21jUEfgYETLwlVnM5i/BocrUOaFbdubKxvVLLtoV/9l9RUAi/uzBD7r1VtCIgLXV0sZZBVE+7CsP0FZ106yOEt1pLV+4EFy+Wn/bIwqHcCC9p8T+lN9CH2NcOPwdWZjLeCiRetn+5MkN32sSd5AIDecCSMqlAcjTSCCr6FZWULKzgmqCCPO+0X7SY45tlxPwWOUN57Es6ANrIrpAhJsWuK6+K+/pjEpseBwjrmRX0jMOu8sqkK4aHYbk9i5wLRCkjyjSirhgqt76P9egUcccasESIYGIQ5PSR1SL9p+l9PtEPUHva8Y14ey+iIpTx4+WA8isnCWcjjMIwZvM2/WLLDv8WBa+Dev/3/9IvbS17Ly4UoJ/TgTvOnH3ufISdeMdzYuGuUpEVazaosvv014IlF74cs6NwIuBleMHqpKm2Fj5jnuuxCSuB7srRXkIEIIDCQgUNRqweHuXfABFh17O1mTyJRA2HO3nLHqriwGOz+hb0hYMt7z6VH5BS4J7nV3jJ0uEvjQwyiG/OtI2FuzgsORfCwSrLU3kbOEL5jsOyxrMvb4xwV6fA5pzDptG7U1GVXoZaW03eUF6bVSMd1lmmaNlYqLSqIse1bzBctH8niXONsZGSGa7FcU+UMN59TzExtdxLhHiCMPTeXtsjxRTC9b3OSXvDg95UbYLsuAhzJNqqQ0r6EWRwvFMZfa1F/qiUOu8xlz5hmbtLlfBcbHYTfuOB72bvGBCB3tooTwgcrfiRs5zdOOkI++yZs6FAwZDXa3/eG88OCCxeVhQ576R5P3w6Jz8BiT02Oj9wo347kvI66S/gJ5oK5fP/37cEQPcXL9fY0UJn7xUk1GQU2DZgHL3vF00h/f/LeEPvCHmyNbHJPvCyHsa4PrDIGV+1zkyiq3OaZ0V/1PevSeKx9Zyg/k/OhgUwk4PRVICGW1DqDMsXZamMI56v3Iejox/VMwB6rq8TkG48KX+MG5xI4gtaGRNQa8LLvZt5aONKor5LfiR8oH9Qy5cHJ/b1Bjrdgf+8SNy4EthFfUu6IpG2BPzDKmnSiFTkEYND2oOpmoYDw6ux1iYD/mhYPj1rWsa3BLMt1t5IDOoXxqpKUmGx3rZY5B2i/LNcdkfvVp6iHpmB4r90KF/RlR245Z/KRHfDz25KzVMU0KHIQBpPD12i9r0ef6wTDOyp0uwU2zWulZqeRZ+ozyOlBNEChOgqT5gXCrjj9vxWO+QALFOnqHc/NI4D0G92BKFDntxjb8IK9jb5ozVy9ws2lCOSF5kruTW7YcWgNiXSMrMIG+OPXZD5DZaQauKSo8VaJGoqSWsPj6cLjIQBo9SePXoS4Eq0e+vumFdgJ6c4oEyxG22RgEByXKBmEL5+jGjKxmcHfu3teq6YqLS8d0fIASsxvjYwGlNMMPTqmvOBXi5FNdU3Kdb7ieJwGMIEp7ZLQyde0CkRAbBfELEnJU9UD+RqN7nHWxo8Sk6LRI2g5mkB41A/AatEGm0QFPlBVjW4JhWA+Jzig9bLq1GApN317+ejguxLaW97R44qbLSd/4/lEV1AezkDeKDPpOU7wh0SIXHwMgMXXv99XhiJV0nGWwKzLT2r64M5ONGirDArqnGVtzwAUZvDQID/p+Ew2FSKuJU9UoSFFwOZWLappnLZ0J/Z5BCbUuLtMaoaqNnWRztHKQnSmu5QGg4u3l+knLnupmAh32azkT4tBrpGaAu4lIx/9q4gUVqy4bQbP3Qadi7l0/1JY7p5HHs+jutHconM4AFz2NLO0a9G/kkIuMWL4u7oSKv3rtzQEJDyd+zOyAmuwLi+rDfUYOs5GfonxnGShLgTeGi3ImOiW18kHbCWnaT4tt1X8YzRHheFMNmdWVqkt8i3wDf1952SSyXTFJO72ugLYKPvsC8CXT28HIqXcltC8I9BaJn+nJ5RZmgW4K15rbzqG3Fk3TWYOx5PqiOySVkxZKwG8/2rbFszPCfy6nNvzY4i8veJPGBmmF683z7bxHX7iK/SS/vDxrM8Ge7bVoZlluOqRfn6Jl6ASolWb05gMmCQxx0dGtJGoUB7baFlx5w1tVPrmpwiYH1BNEG6Lg2dAL+udBuTP0L2pwZyxFHGuSVi97JpB2BJQZTshHCU/p4nZ1smIjZSmPO8EUYh21R3USacn+FHhJzMti7jErjq10wBsbu7gGow3tY2foJhK9gwv5mP87UzfZX5/5YSXP2X9popFziAEnY7icaDqoJP1F+T+AqoPYGk0qh8g8Dh27j9UanizyJ1c0asTmuFUk2k7KcGq0c/j3VDUiccP/EUs3wa9Vue55VvMMDnwABkh91RwN07jWWXMeDzV1eO1WbtLc41RS8k5v6x2QXG7SpYw48RTKiixQbTwjzpnwrIR0mVqdlqfb3dY9NOIAWY5QDnr9l2XZgwwHjhCSuu7nx1oILmk/bn5b7Hm94tO1JvZBqkGwZdd2CyBsoHhzz5UpAp6UCNCNWCXvG8JZrLJOQSXBIaUmC0kjzq+/UWs2oScjnuMf7k4rsVLjeRCrejpGKyisiQEbtRi7/gNHTpd32urFVBGjEfwP571eCL+o/V7rGVudrHVxROdbN9fUYoR0iphhvTYVS20Y3hSCs04sFXAqAHfd0NAPQpS6R4KIEzQ86H4/N7445Akjf3rEhbGejmX5n/slP2PqVnw413eDiH6CNNbwVhtaKQVC8XKEGHB7apEJUrV0CesLyZqdv4EFQt0Pv3ohELWyHHeVMttSCRGteVZmeOf1FcZ9WLkwZL442OtEA/PrVP05wZLh2b2CC25qB7wBTsJ3Env3riie+RH4us3njSl7NnXI72AeFuitFVVoFOZ4DPxv+OHfb3ds4mqC8sFLcP+5Wb/pwV53L/Bp6Teaz0co3ZKUTkRNegWVPXur4Gkrg02wma+R0skg73X8HgUq/DM7qnRx9iUg/A0adwbT7ZfntH+G81BdZKCdfTmpxIcIIy2zn/4KU/f5/lerylwbCb5f9f4u5giLIImHWNkflRVZepLV1jdkeBt5VN30mcQuma5o3rn+J4PWD8rkrkFWO3eht7dQupqyEw2aQTP2hBfEidD30EsRRkQyXgQLnwJpFe44y2pVGRs6ui9Y3oL+CLkPjPeis1UdbiTV9TdgFS46icA7LWNZLIHV5jlC1SWKgTyMbmuzFVNyb5KyUduCoi7X7URnoLIaqi/n1NdPDMNq+1XCqglps8bpricUWRKvh4K54hm0QYRft2T0pGxmCyKEYCzIBl0BJUXXZHjZCsnM2D9sSUoDXCz3MdgxBV/hsvNAH2xNRKiqluXaRDPpM7jaciqgXIH1eoFbudxeUxZLFoQkenCM20scbKtjdGhuX0MjVzTXLB0L3unSeLVW/pKubGl85v2meq6Whz7ENsYFcXw2QPFoHB5Jou0H2g/b58DIEr/6P9K9a6EkqDdZasR+7b8Sy0LVkOwQwV+hJsmWXH2LFBIOq6MYf4VT36n99eLu3UYrNk0DUrMIwOn2QuykBWW0CD+ivFch0UoJAwU2+fKMbkDwZ4FFZoA0Cdq9GrnOwXS2Ez9RN0T2ro+C/zWhwirpfBwg5YOvi8R5t321SexmgMm/dlg60Uv6w/SNCmON0jPvGvKYDHiOHaCBjmHbnhUoyriAL456HWXp5xA9NB1lVCo8NcJgJDA4lhj7LL0DQLNb6WwKqJKYS8dYyOM21bFUKOoirRrIFild1NuWHWmKOEMTKl4K6N2P12BaQneSq4iQU6spoX8C4ErVhVQps5euWULsqR8kAkkX734qyAsUCgBhepILni475fp4GmdIXYp+G05D/zp2ho39KXXrjfQw/dIFcsn4toJyJ0qqJTs+yd4kmOlk7TgYtfPi99rEjV6PBL7zxOZs6VBc+1iXECpo1RsTbG0jawS8SVONuCOWZWjP3xXOGE59AVEEAIlAHRm9/jSUPGU1gxSwGyFfMNi1kS1wL/meZfGb+2v0skEJ/6F/F8rw40dtR6IobWKGwkwq/WoPVfYXRTdVYnYlOEH/m6nj72/p+Pbf5DraZTTMZhoa8SmxdjiFhRh8iPMmjX30OTOpvNm6jNT/w+E6bBxm94L/ZbHTHcmWIpqCUkCKFAWsujqBZMIpxRcQpWTXQ4YjxQ2pzNHE94EsB2bOf2rva9nKiE51uLQGoguhPxJWRsx8Oa94d9XH+VAuxOSodeKIJVNo75Cmnmy3PbSTsKjJsp8KsBxMUYeLIAKrn6j5heZr0n3bhDlY2hFEbg11S/AGXbCpbaYADgKny1x4y8Nt/BV+DbBBIYLN3BNnuV8RGjyVXleZEJ2F2dVsh3/oFmz9yKMrIaON1a+aH4KGH0KfhUNZlSWtVYZVqXZG8j46a0Mf2bZs94wLuf4tYS+0699lDVcS8AjXYXFQLTSkzOoR4xsTntWGsONUBwgaFPIraRGFGnMkg/+Whm6u3LYPmUMQQkWgylrfc2+H6OKgITvc0pVvKfxLlAY52xKGdbTnSQblGslJhSFysof8/Z/nL3FCf7q6/nL3FCf80xyoYAfmMBcmcG94CEmUFKMtn2i8uCr8dpp4yrDEvnBgqg2D1GZTGkmJmZEgOLAaD6KS+ApfUdlw+eYU0Ex24EggysIsV60rqCfFbC0wycgR4+6qtf4gJjX8YT/Wtdjj2S7s+ye1j2Dsyyb7qh7GQfvQh6Bg2oY1Qt9qc08ViOhedA7582hurOrXH7CFnTYfRgibU472fvspLQSvhLItSGOOEkzS5m7dbUgibAE/NcoPQHsLASGvIQO75TXv7ERo9LX5ClNAmPBmE3Lttk5eh2AWEF2quPsQsUqa6Jib9/v05s++nldFkMcPzhJY/o8qvNGnkk69EDgG2IAuNZSwDETDKxAvWnWZ2HFy69XV3O5hDEI7LX52KdIwiUpQNyZeB3vYvWE+gpPbB1nQFa2cZ4sMhfNs8XXoDkjQj8/+ve9t4U+YKdxVarTVTpkm9ElTw5OTXxCNmqwJuq3vjwEyf8frl4h3DQJliAPNGChoqOPhU3ycJhWGld5DjDBXmEr3tGs9zI1VKwKdW2cOPVHWVcKg+lqINX7Tc87k7d7zkgZH0H0fFO+Zx61QvLYN/FUAW5zqhx79XBfgzO/2TGBcRi68GDhfw8vo95+QLBQE+93jVD2+bORDWzP71HqTW4xFD7M1lpUeHt0xO8ayHJ3naJLpqZ6roSx5Lj8ugCvMy529RKBVs3UytYc8QGJJzym3GqNuPfAvNaMZKwoQ04O6/Sgkf/4aZP1psGnoBra3njWSJ6MLHMZxj0tslmPx/FbGMQzpSwpO6FmERPCw4Y7vyuyRPJZOQFep786hO7TtHPUbzupJkm5f1KTDt9/TAF1xBfpOFTj98Jk04qiFm31em1Y4T4faqLsXo3esGsJ4B/xb6U7s0NHQakf6WtS5eT4oi29DZkky1xmfqSDpgEhpvcuO8Iyb79xzwfgYKM8BKJPqGOrfjfd9P8EOOCXSik5aZnd7uCvhs3kZk0jW9YPSsnoc+8+YodrsQrMNrescpEV9r3R4Ytt5GmjRSZLo6emysXk4tPhzIw0bjamLMVx4T+MMgNKm1s51nR7dMcxr28aBr3n6sJNCIR6U8F2naTQm9wW1IzHyjKatZbFJmqoYUs4FqXGHGIGuTpGWve/koiJvAUaG8Ii1sV7fc3He3WPYeE5YCjGwWCWw29WjBsoXl2wThjTHZmnv/NV0ak6vaGDe/laPN5HoBdlpoA70AY/FWTPRsMiokJ+lYDDt89Wj3C10jWhowPIJHrZHNeWo3aSVSkcUHf2uhJFKs2HbnlrlUerESB/GzlH9TyY4rycZLAShpfelUQEbAo5v+jTO1hrGpuvHbY6GWyGXhSB+j60sZomWP64MR5IFQPKzgnye/E5gTnqD+LmFmdPhviuW59jfHMMjjTSHsjGEanvA7GWQt2+SmqGTsQZkUVAlMKYhY+UqKW2o+mNU+prV3+2Pb/gxIp4egEwJ+A2cg5Xvo0t7wYBEhuaabmvGUXsfy50r5VporFzDi4q+RdkcQ2TK7CSw0eeZ59cYfdJiTvltC4ojRmbaWK9kMHJ5Ab0UR4HO1kEIRdzJzzFPFTFfa5RNgzB3ABfbkzCa2DzGsFCQ5aLxwyCJSNhNB5YYU80VKRTn0NHb05ztqR5GSzJrlcJFIZfrqSGBdIJgJZUIVJRsaA7D94RIGDBKPhpJ465Bunodt5Yf7zWm7G+kN7lBS7M2emtakdNkHQ4G0Nu6rHRlOZr5mhf7HPRfv3UvWOIp+lkP0FXjbOZidI2y44BXff2KfX0oUqx3uVTW3a/Lp5U7pUMH4grwsi0jwcf+4bfqpZnOKP4SWLqkxGPtYL5Btvpm7g2QVIjtzb1Nn1lrKcU4cYK93bt/M6tYa6REqWMJ3boMLMZEYwRFOQMtyzFDZw1Woww54KjMHGvLC0OFa5+6azajDH5SOB7RGNlL41JXMgtdzTUfZ6ieE3pz1t/Tv8Li3DdehflILOFkOYjWOgQh5QE6aDGCBiVg4UBE/P9JKrNOPtlpsrB5kgc0OYeWEpiw6I7zg/h+a3jrcI51D+So8MvFPYLdJIndo1+9eoswXULPyaeePzivyE1DMFoGWsMLdE8M5nnbGCHPb79eEpqygG8gA7nRzaNysVoF8NFhHpS8P4T1E7zMAJfRGKnYz6vT+TKOwo5FjLW/ZzbhHdyVR67odVQXSE7Pbh4Q+e62UCVAUk6vOixBmMcOo5gIQodZdaHZbz0Aj5+OPXIS0zeR+VTCb47ZmD+8kPYdhc14Ybj+uiIp0XUQ5r4+xv65dz3gBF96zQqxz2m5uz1Z43HTLqhBcreJ2QJZhw17zcDugkFasIu3mTkcWqYKPP5mZADH+IJRv+fykQyQ8P/+8gwu2YpyMpiePoFDS1pY5KP48t3lDxVVDn4rAmW+DLHFzCUKmgwP6E+UmY4Vhs9VgXlgRLG9TU2En21CeyL3Kbbun5dlM/FeU9qhArR5a170np4gyLsecUM0GA3lSBm3irkgc/YhvP7htJXrihmD9gRfMsj++mgfRSSsrx6syUC+O6eH1ikBHWap8t04Sv6w90euoUMkSEKbCsI6oBOAfM72KDZ5E2hrINVv+3WW1crj8Xo6KdD7acsjfPev3KcT0vkiV7V7LXAkZXDM/W4f/CvbVaF7WyC2yVr6HFTBp6h+kZ+jLs/+TjaMtNLCbUTS9RGOLA4eCm2ooZy6mIeHj1ODq0PAJVO+NjfN2zSVpKkxE6PiVuMa84zsRdK9zD5asGfJ31U5eOeohGJE9kSXxGfimivaCB9LruRJeqJR5BKZCNYgSwhszSHpDtCNDiv/d+AvH8UZNDPAebJTxCYxqJQSfG3/bM3243gCmzlcN3OrmBZfmJ669vlSTsZ31nBTFAfn6VUPrYxk8ipYeKxZn4BTSxy5P80R+7t91L6FOhh0O0yd5DIkDir/HkMnkL4h93cxAfhq5iKODk42lngmd1YvvVhf8uJgdqyI60yuclb8Yh26uulfONToBQabFU79bagvp+sjjXM4UMstxJOHimCQKgwyENDcugzLBChGA0pREYN9y/VpKcIg9I+AAAhQ6bupeNF1ib2ykuhnrH+emKYEctAvV9dy7wGXwM0xbkF+vGpe2ZiIwWr6rXk4j3z6gYG9c39YlQHWXZKLzNdugUr8oRWM1Rm8n3+cy6ovouhEMljYzUrVQ7wI78y/hMLxildnn9CAnty9kd1if30pRGqbqR9n9aJf77vlVihhAjXv7cDsMUJkmnUWkj0nr7ne4XJWZnyPb9us595ly1SUo4rwHCA1u1lRdWkRZE9W9+fcL/RJHfY3yasXWC9OYJKoDX+gYiQQL9OafV/7nePFq6/XIgcRJaKgMebi/XZK0tpNtJO7TayBFRfhLBYjyFIUeUIInBcpTCu4ywPoxQHefhDPHoZ+Km8S0EGMLKWHSQjVvYnqq4auAfNxpVUGw5VGQktnkf28RAM7bJf6skFK8kTgb05SFy250eGdYWLAhPGkw8ou14tsL0by2oqakGHemu35AFCEc0nnlC0F2Kk+KEnVHzzC76ukiHHiM1VFEh5/7sfpMKfK6Chf7tBmk4/4qh9GhNfm7a2XNVB2vEoVoolaOqnIhiwcd6SZ8oFqTcgj0iA0dX9/1kbiuhoWuCW0ioe+ynmaR+vKgKMSX5lLeu1LNHrKs4kwQkYtgJh7/C894FW1h8AMbdIIlcKyO+yjgGSm5u04LZEKynYQihH0AY2cGN+Ljcg7V7rqQ6RkmXZQTnqObUZTmrrMRzRKRUf79s6wR9UdBSWdwM/7hW7WsuWZd0Tj9BfrPefpQrQudoekyfLvKo9BR4vp/vn8DYTM7hc7xfW91jDMmYErko8i2YfaTLnYqHuX5fT7FlOSmnvZzLmchcF5XGA/myW6IbIy0jjc/sMy9UyspdrhYs8jkVP/GNeYgY/LyDczdriQEjbT+G5TNJ9iXDUXmPdUN1dOLfYlLn0hud2bK1LVbvZqqh/nTRA11HUyNghmj810OJ2S4uWc8B7AFdhGS3iHumTsWwM42VcIeuJiVQNHIPi6KnSBWrM6CEoGzhFMBvrtufmJMuyhBL8C9AwhYgGjnuCJWLxuoNZYgAA9Pk9AB4MOmvSFZQh5raSvJwd2cB7UoQlgsMP0aZ6tYKoG24sEG28ArEG57HwDT0hMcc25BDpL5Me1fgJBKj7sMPIZaAErMEycH7sIxjJ0oV0O55FVuQFb0ammb44jvX91pbBR+fgx3hMJBBBNF0pxs2gFHNyBWTAfP7GV7XiTN2bdgr/uL+8DmyW+4zhO0Djl14kihWxra1Nv/8iDIkVH4I4pBgo+BUXAVx4yu86k2RcZp5uSAzgI+15r6atICMgK0Y0strcWsvISIW19dPhhJsmgrAtNk0fJf3lp2uOVdbFQHOYVCNgEwGpExa396d/U4hYOCcxiVBCS2ygK5E/W6voFPY+GcXyXMM/2JnQ5hK79oL1Rep9BeDghHfPWZC72RH8n8S4iIdzHvE4z4mJyFERxu2pGSxPOQXfJPk/kKI8u5L9/zI7vbBcFI1RS7AFltKy20EAksXKkdY+9wWyACdhC5FgLDWbW0yYMz6m2RR6oVyfMMZjfvlzbRYQpQxg1lHQmvqhVY3c9F9oY9humcmNxx2zFt3Csyjl98WPhcyWeAa1dWBYZobLYEfguhcBjmEuSEGOVM8mMTWFM6rQDJ0J941jQ9I6IrwN5DdBbnDGG7CHUUQ7wmUw/K8XWtqodisQyxc1duRYIAJrX+PAlvNCDFnFGCnua6XCAflBoKyLANqbUe3phm450RJ8JrUlR6uQC2/MTJCLcBPGGJVxzlRyc1ZUClMoZm9DDfNm+zLzpN7rSwm+XU3jJvII1uNu/VdR6iCUZhtbyHUSCUtrQfOZmQ0t09MujVIUfVRuP+cy+KYStbxdQWoT7ygwe7dG0vU6ZwlxXxsKBDbl3mRBvQNYfNAYQuZ5vOcVWSC86J1Ni7WqbpxcaCrPTsBxAEIPPoBPNKsZ9XHLZkWi1eBu5SExthea/bl1DmthgjfLyxtFi+5QPwIuddJTjzjQL7xGpelzcxFjWRBcZojnD2ia4mh50JauDWbTQ0vnQGa368S4Qj4osRtVAx2m0L4sy7RcKwJcAP3X1f5txMFxaDUdMI3/iQNQ+QYzbqwDA7kZLazYLD1e3FMY6+7iWTr+D5oPq9SE9Toscr9iMDxvUM/586yf7T+4TITAXMEEriVF0Stf+3H1QZjnlJtxFe9j9BptZK3Zwxu1P1E0dmVIPM+mBLa/iz+ftr8UxCpeXChY1Udk7uoKFZcMsts60NK0IzLn05LdaeFme11qBi10vYdxBFS9V/naJZsEz5xp2ZzTssM/GkU5MQ+K0966ZjX9L4vDr26hRQZhfR8SKuFvMR5WsKnb137MxpB2tPimdV9f3thxCjULNdOyLxEPUTjXZsL2SUE9FBvgd0rU/HTM9Xx17vgIWEsADCxlIRwT6q/VEQfKQVej4Rvj+OE0LDbHXZLwImB3xqP9ihfn0y5Y5gOK+97eOTDSy6OuCYEelDyqlS+T6F1hUh21l17JucOZpPLzLQZxxeEaxbNsaZ24WIHzqCJyhNvYAqHOR29ql6dyijxylK644/5AMigFlYb0xrB/anNm2T6B3wQW2n/yRuLhD2XA9wHt3WQpxr9RTQPufTLG/jab9tPRn7V7u4ki2a5uOfJXdIRdNL/ARytlBME9r+d4Zu0pq7y7lH/QWEKa8ckbwiFOMQe3BuvbHkdtA9GWUnmudzeES3fRZxTHmx+hAMyOdS246UOzHOjGE05bQSrN7gH5omaecdusp2qu1IVV6x5ugnqYHdHHdXkefYwP9Xr37lkY4M/yN83ADbW5OMdVeFSTArL/3xFXDf693QkthZ4bgdNYGV38Jog3jBmklBd2V3IMi94PzHVi42/Up+ay5G3X6eNBQEyPgsn3A/DWwTSRp6ONT2mpImCRur6EpXQxMBGyN6dmbKQWCuK+4TTvanJ9LZkXa3mQp3AZ1e56V+IkMXZ1sUivY4TL7p2WEd+9c0RV6F2Oagq1AP2zGdZ9IDxQmJIZmoI2w+tT8Qus8sJL6vGBq1HH4cUYEUTNomscsrJsQogrXBHN0F4QwqBNq7InxLbd5vsyOVk+o9xk7IDe6p6nzQIZyGrQvVXk3wJC0Gyd8nZ+ON7kwgLcQxP+3O8/WWsNehtnP2mVKF4VHlF5uZlEqrpvf9TPVpikiPu6/+OxngdndNtK4q4ehDmgetX/TbC5bIIJ9gLrH4CmIZNDKqt0kqVDJpbXq3aFS7M0UsKwd4Nd4GXMrOZNGyBODDMmZkecCtiswAi8V3Lo7EeGf9tVAIdFgx8alZyxh1mUT7p6rBTj5gIcFp1w823kGTPwcpOGmgU2ibiIS6JttUzrwN0CG+vG5TMW9JRc53KGtvmF4RENRBQuIFcmg/IM7JXilOogn0BnY7d7XXdiUKCPWC3uJSxIVSKrLtALmfcXQ/sfQV8uKI3C/zQiqUgx8fy+zzZDQ2Sfib+m8Y9loA3cUNh/60UXm7UmzxOtDdZB7EUt6TsRPmM+jVcrnvkcOP120EOzpuTPSbGucDlsmoICiLhb5+KM2nppRXIZzldLCf4p42ZWgDuIo/ZXHw5HfUVhCoQOUjLRzYuNMx+fv9Lpbxjn9YxEd+K6Sj+S/U30HY8BvXrJMLBBwSiLZGBNmkwWcfsZq7eq0NOj6dBLHODwNLgB/jAdq6hjFV9AOmlHgz5PgcNYa02lkrgXDcbrKomYWg4lawTgYmAwfvHKW23kDfgqdaN6TRC4hBA1c0PH+HaB5kykvmVh3/FH826oXwe1CKvCJm37A2RnAbvnXFcD/+i21UWDXcHl/chro6ge7BPkpDMqNHwZoBkMpt6Lnl85pSNGEWtMt/h0u4aBzr77CdMEHotNz4xWiLPXq6ttGixCZVcHFb+HB81e5zdHogSIOQrRzDaNj6DqO44/KQrzNt8f4BYfjA/Qcw//oyD+yTOm2IY2iGn6dqdgHEY0GydNqAOnhxJEWD3BRBjHYb2NlzuaIvvvscErn1HuhWSb7Ycv0y0J5Vs/TbyFTSP+eH1S2psRHpkPkEqCrI+FoHcvI3P7a1lUjqpVhTohpMuOV/UghevysvmNv2PSZKqj5+8eg8DGOw6+nFMpqsMIj4FiE/Mpz7LC497qCxRw6zAvP7DH43W+uFhb6meqzl84NA7Rn8km10Yb8dkkLM47vkPjpqlDcoUBupx1ySVjfRoEmXv2BE8uc0edH0zQW+bno16WtJkpIt8VqceORC4kcwWwepmPR6pr3YRr7Dpe3hfFSXNGaTkT/HgiJP7e9fz8arq/Wu5FKnPCzNDHIJbgL6A6r7xA+GNA9CCndISw90dqwhysaI84hjX77gGoEucssDfyCDnkpqlOy5eqabWK3f2xPXVmKd9VA0g66u3t+1aZIenq8NUl1env6nXTf0BuxvQ9CTXwtI4huTjLKelchAGDRQOtItHsRFlwb2tXkxGhM5OuJ1M8aWpUVC9HMbno8Ive/6WZAYepcOzkYlf97SzUV+Y2yEhK8T6A5ol9QkirbKgtoOoL054M68X1qDnGrKln+Hoh/r0vHpLfZ+ZWbXtF6QTo1kbmXFRt+KCLp6ESz84W18jnLMIyIpj3fG+PIEnp0DjDyIaJHlgZMWDUfx2/aPhSus4QdoN837mYVDiO+NqiwCw5+u8IX+Trr0B3lEPfmj02X+vLwH+oZwAoZPf/IgmP58sG8iwDl46ILiubP57NFWTShQxbt34alvhc7kEve9Wc8qvF2nMUd9kLT8PgSWbUageNOODA9iMsBz61nVJy0djFJSIi6Sc8U0+MmwervFNPrzfNRV9cGekaoeMHDG7IZTtX/c11YvvMAT4XjtT6Wf7Pk8tMhEYy1Cwyo/xEOMB9XXSw+GCJYNSH3MYmqe4rrs6DerK6CRZ1Z3LWbi9fRjrYQc77fWMFPCWbet4px8+KnZPc4YZhElTW2q9q5XfSM2kkIcTUpFEbdCH5i1d43wfZ0flC+lZtKd6l9dXdo04kxrM3Uus5tb3VqOy9xEIWB1KTx87VdpAWaNXzviXAbgiq91UR+tJ2T1r5lcAhwbyCghPF1VpKMwFYmrlWPco7yI0WUZhnFQ1OAYCcRQ5eMn2YmJOwHDnBABPiZXX1BK+wI0PfjacLQlfga+gXxOVJK7atxYUMcaEJjcz9WPf2478LurNqPp3E65OAlY4U5ECUQS3WGdzYLwUF5A19O0bVNVeHwyiqLMibIHFL/VafSbZFzLFMbnaRRYCynV4MlbMxgylykO7kQK+RxzCMP6xk3yoUajNlKaQTfAjbfCAXSR3KhFm4g/yWdzx18G2Sxd1l4lUMlDg+b/OzaIr+DiIx8YMfU6qh+pbXDnEAtJ3xMee1gIPFCSHkkhBQJ5flMHqo29PNOsmXbpw7LDZfFQammMOy9fKNdnDVcrYcVMsrjPnjlS90DmONg2iSVKvEJWcnQm8Ofy+NZWDZ8PE4R5Wi4laqgnq4RZW3OvE9LzedsXO5Rh4vNVGBk0uiQ9IhvKB03JHqm+FjW2I2tNpSWNyV7HMKol9iTlpwtu41EzpAA8YSLm8bcwulzyqlQBdYDRuBOteJ7alxXmL8hcx1xiR6VECuMiDSjlWSY6OFyVwOcAygEl1vgYeRwd069eDxL7WITtWWbMp3Mh3Joc9d9MAXdkukOLaSDqSUkRFzNC0VQ/U36SLkL2oWfkqjlp4VW0KX+WsHmvlgb67KH9K7F+Z3+Jh/bhhd65U0tHwOpyZOlcy7s/MeiNtvdkT/hKVh0Z7/7Ol0Bkry0gEOt7qu+XS961p/OS2rKW6CHCiOfvLxdyidDgUj+P8VwxQVUtA0MLxGGdnzzFp90T0miCR4ONGZHizhPCk7wkttiI61uLUcJbgy04oJ6vHMFwpw7/uIu+MK9JTNJkAnfPk8hJqbtA+l63jql2tNI5CWDhj89YvEO1rDaP5jspRRRGM0ZGwMgMW8mKJ9pB5iUB2f/6Gx/9EDs33KNOvLHj/npUP0KLn8T/2zYkLBxCO+LKGvfQnSskX1CWs4fC1zmTvti6YVBsnOUDs3QKtVDXPQAs7+4c2vfCCyod079pxO5pM/fAkVdcq/ww9VpFVbniQpemRJTc5a8M7SwxFXMRGsVVYwLktZ89GE95UolF3d7fa83czcrPLo8DISNvrI7outaFZo8IZsNqLFmFfG9Q/V7ILbaHNv/kSXiK0/gxRJKXFGaHL4P0p+F5lAQhoFFi8+1GDt7OPop6dWVtzMDkb0rAT3Gq5pfMUZHDTIdBB1B67ryy9LB8EBMPNBpFDoWtilWVPoW2i2mzA7JH/DrLphf4RsF98N/4jn9+s+sbqWytiYjYBVPbrAXtxUOnbE+LQN/msaVvzRce7O7YIIEH65J9N47xoDGGT+6nAUalcn7cu6CattdLk6h1YI3Z9ZF8gT98rGzNjas6/UErQt3QZyBlBTdc3AlQ35gw7LW22HBZHGGPJ/Rn08vDEmGftHcNdFaMLxuUrlDRf2t+M4BAz2BTvj4VfhdXw/Q5DRTaaFXj8dTOFrPzLYgftWz3tMJGRIA1q5uw7mLBDpzc/l64a7zJJjq+Bl12T77zCzHgIq+W8eL63NFHsTK7Xr2rmMvmRV9qV9e5/7GyM+cAAOWkOlys7ejfF4EYDWHjpeY0/puCf0rXiRSjUO04W94tPO3WpcS4LZDG6P5S42ZqxPG/Du85qzfI88UX/Zjvuk65pHY4gXDbM41xMI+mN5RrTVEb9BE8C0eCPIIkgCC2kBE4rGh0xe6+eZjiNM/XC2+Jas0Lal1CzMtA79oBDmYjBK14cM88G3wo6Whp5vejNjNlPn+5ts0oaB9HaB/E3QWXwfeOMaOfdVO2dqjYaONu9uhYkO9HXpDRHf0WZLjVo+wgBrdMtwfMhDqhmWq+fJWxLL7q/RBRMPhLE2FtENo7Hx9W3ptqqUpSu75jCXhsBlQhwWlH1PaJzjKX9qkHditqtRZ2bE/CiYeNIGVWd0b8SG2rM8yYFfo/5ZvA+BJ7YOVC6mKb1YBqJnmYSQuW01WEij7duw9QBtW5q4RajIgawUPcuf/B505wAh8HICVwZWMcy1XdFkiK7M++/XtBhjIagH6KsYWju4Kw9KaZe7hHf2NF3+bh2l6914NxtJeonEdeYan6ZnYAF2MSHexopCQLoIoom1NvK3KZqk9oBvy0dbTm+U+qM2dpg5xq68JXMG2iHZi35nfknTUZPTQsfL5wpQwvVkvJp1FLdAbTlvKU71SG5DRQxSmU429STreurqTNF35UeBrlYKUWjWF6H271HiR7pKnKaBXuMsK8S7e1Gfi2onv5WkmtPBGdZwKMSrEGAiwhl1UcGpezT3N9r7DW1g9AYx5wlxmj+AY0lGdlnr21XVCGNtfje39W9keXE0qpNZykbUwEXlU7HJw0fJ6DHmG++gItwLMs7CT9P5qVQ8jighMVn/uIkFvX/GTHLxaT2KA6tm91bjPrZICJAl1olkypC/16fCLWhajoUd2daA8impjlulctM4hwAs6DelnzFNXZt39ghvufQekdAWwLE16eq4qSzdAq/AtlUvJohMAbc64BeddzHeBu5BNcSihq+BX68eGU4L068HQ88lTJxAQ64/b8Xr0IAVL4aB6+C7ky0GUSr+lE59rC46eAxUggMPQPElulsX7l3AvUowODC/AGOKLkt/nnz7KKrzt0ffkaz9MOm3bK6nviVoqtkTWivabLLE3k2P2fkJYIrMKSpSeOCLcWGY7gLMwwzKFPLw8U28/pV20EUfX8ovJQNNjqZlDFxJG8IJIy31ZCFGgLvTKptlewWBPGseaXHQ8Ud1xkZ1GgFXn1SBVrSLiFTSKRL67ns4RfkNzD9oH0Bw7GTb+HDYg8wj/7BjuRiDTv5wKl4qIUYeqkMSTWlaz5jGF3s4OKZvZII1AKwI/ONZl3GIk1H0JT7vVhhVhi9jkAmmFiGZN1BuvIyGFj4MjCB7B+2LcaUwy0uWAU54GJWKJbLlELFFWf4WEO0b0cm+IQt7q7Ff/LFbEyTdNM4cYwSkrthJvpZX1BQdVbtcbs9o+OLylrN4gOizNzKcnB+ym1zaZwp0SjicL7Tr3gY2aeCmXtuKYbM0otRhFvYMnA9ugCdDrHIH/+6/k7Ofv15HttEcSILh0LGEAxuaSHUXwBfNqpMea2G9saZUAgneBE5sU0hW7e/qHJNIZjIAWIQ7CRgOqyuyRKK3qLHsZl31oIvpeKjgqYvE9LqxZT/vtQgAXKQ2KlFxTNN+p7P3QUeMLNgoDyAkdCtXBtMXtGoax1/6EHpZL/YcsHI4oX3jdcYze2I4jUkhmR53XJ22pff3hWNyrt13/yzovbHXX49LqSycd9WLut1gEfqQVxA+upFM5YgzfePW4uP658wyJRRZfg2sbtbbGM44jxzwYRK7eBd+i2CPXSL8R1yF0vsv6LmchNT5/Uun/RVOw3p8DZsaSUMr8+p7hZIU+HaDif8qcwMaXcBAdDZUvTMx1hxhMtTd/XHk5u0bRhchXC/wLUTxZK5MH0IpFlTOWinrKatKEeMg81VlB90dWeXsvqoVilEnOYXL29xRs5Xun08yn1SVmMYhGGGQjg9ooSi3sj9F7HV2mi/08WF0/GlptFOsw5kK8GYoiODx2AOT6aQUOB1JW3+OjYvPdXS/S7c+WknhbMKpefOMfDgPHy1OcnBXBxieUxZ8xRFvpMUpX34uaqtdEhzlv4ddyOrnDnQOmGYSJSuud5NP7weYBQAtSvGU8q0TFOWfsHev/EiAsNU0yaTNSTDJzG3e3VgrkO8QFjdUUVhsIAXUVeDeFKYpwzSkpqjgWkikhgltMNifugsYbOvcyAVzJy4vsU5Vuwh5UXtlEzroQXbXrTrqA049SeCvF3ziwCV3QTeZ+xor7JIZO9WCTXN84arQBX17BSzpXC+Lmqk2U6vGci6wgjOSdE32Ao9aCjYMZplDR5l+xjEaH3vGYFASroXmgnKuXrvk3YnihZI83abUFfQqwyj5MgbaKjyv/pjnsLNC7SDUOCA/UDkpfM4mmGhEpXcV+iI+v4BfwEwhF9L4Jfl0h8kkHUpAy13ss3pukHpm5k609wAJId2xBK8U6WSr7lHZ0fgaYonto4GSm0h3kcV8dgDBiaXUi2NSBRavw6yPe/1ibxbWPAAGtLpmdzTjqi+jSSEJmRwCEIAYpdX2CmftDVoY+qj5Q83QC1SK/lODxTOrjoBgy7RAHKvzcyDN3lYFTqmSaCdO8knQTKUzt5DCfbsLm9FmqKTzXcQX8n4vIpdslaryCmsXQeOPMMOLfJQZpnrt2Vr00pPyVqUS5RXw6tZydfOB7CMYqHipajZwD0Z99HsoDYXhrBnhlkVccPajggWKbrWdQ65jJCgMB1cxyDTJQnNIYQ60jxNBifLkXFAKILm8VVCA0v6BEj3T1eIIrn7zq6MtMdmV13mwfFCthCUjoRckVK56ifzMTVqTzqe6EUOjLDc+z65W/JbA02x1u/Co6L14E20d+OSTlA6WLepA/LoUxqlaCtESaV77t3EOJX475wao5y07vtPXNW1uKiId6MxRb2mRWAVx263cO9mOTwrr4fA1bIAVF5SxgTyAkgm605uUrkJDbFYaOYtvJ0K+2C/PKqErPM2JAzvjOX47+qFq3th+CqvMuQ7INcHWBPK6uUs3nesxxP+Tz+gNJHnAw4lY5wQ5YfcsQE4T1ZeQ1TW/g02/G6AdpADV2uA0OXNYQNvYt/t+QkxD0jAjXx0Wce9IgxQDTPQxdI7IyGl5thaREeicKvIIs+GG6jlfZxlwuAF7anrKQs8dk5kFu8SbqU+lZ7nf4Dwl3u4Eywok8eOLizcrQ5mri/yyJHkujxDkEakfxaRqTUAEKyBH8+96FIxfVQW8Fciv+qslqq4YTmX12YWtCwS5W5xVgvYps2RX6ApRok0kV8sO5PjvJ66sfQUJufub97x9pjs9/2GFrfZmERVFZNvut5uEP2HcC/5MnoIR4Dnlxriv1hQSLTSAEPI7Hvjsdy/9gPMhjUreF+TFTyGM4Fyi9ZNuFoTb1kko8D6C6gB1yj8dPsf0pjj2wkQ9irmsr9bO78XikBT2vqMpeOaVLk1fSUkyk4SrNvGQ+klkjkv/l0ijN+leq3I+eL8pIK4qjZiMaUuc2DGGORZXwMsbTqgUmSzqP6JzURA2ZrzyHNHeMEwBROdanmB384laOMymz0WcZ61ToOtKFQMIQ/vTG0XQsFT1hSYQCyj+oTI7v07Ozp4xXxXdO4NIVlIL43I2NjFNVh7rbhh5jOWtKByJQpeywyNtAJUAc+tNvIi2YBdAlw5DR8/zLx11RB8ufQHQ3egBymlacZcVFuwPftTamMCV1disFCuWk52fv2eJVHEHgVBOPMnBNnt+RKmOa/zoBCNdq2AY2Q/gGFlt4ILxkYrAGbaSgI2t5VoC7NQWfZSmANYEltL870KsL30rga9Uce7RFev12pHzcTcF+4I4xrlVtHwdSZCaAW5IiJMMFhTLFU4jswVBRnRQRNGOFpNVdBlU41/fL5aYCdSn3/WDobRCtzzSUSfISpRytdblkodpaSq6EqX/8vCVL03Hw4vH6jPH5RzOUfIxYlctpbQ1Tnwh83mhnjznb8AnRXmNROFQGygdZduDAwVh273MNpSMUzmafbwoGj4gftLAAAAAAAAAAAAAAAAAAAAUMMAACUFZZADnB+YAS761JjyOWkdBoh4ZDJxrP884CcXH63/ivwvvsz5BrIpfkpNoi4/+D5Kmg43sCZ0ROVXTEecjOtS3bjuRVR411O8lMnfRzkxP5s9m//bu6Pf1EOHFDlFVk+rHLLO4SzJuS5vzwUMFCrrK7kWrGNNc093KnMZm2nx9szX02q1tn+/DL/945aIQjLN1xx6+/vBuvGEUr2DsSMGSIipLc1zah+AvQBeMsLeAWcJqajvk9ZKjS/VZY7GWMti4YLoH5XpCrujA6+hbvyq2ETCS2m2lewhgcN9Pvld+IFThA/gSHa9PEArXGtVelJO/Upm2Jr/m91TF2DFyxqBfcMfxS57OF5TQXt/wZOoX5+ilVg+FN+Pmh+GNb0v8ObzwxJZBs2g0deiyPmIM8UUZG3lmo1rBzQAnS1N1stQck9HwuTc0+U2WmCLVaIQVjO6731VYGuuHhy1JCUbwvaSgZpKLDt8LXSssilvcVVRrXlj4IRf6vbNoFX6uiV/0gwtXiTA1rrBSlPd1dcyn073LdpXdhn1YQlkX2nlKl7k+APVCvbKaMqgxdw61VYphbKBVdP+m5yHKNgxA3+Lrrg/YrABXUlHa4rfXfSkVIw0yxpLHf1Z1xhowbBV1cVQ+vnlDpbBOWcm3LNRtu7VBq1SZcU5mzqnM+eoxC2IYRDTBfuog9QzxBdiwRcAT3L+cvsIXsrchVme6cXVpzsdX7FTOQiQtUaJcFQLsyP+s+j0dSH31LDx81jpQrjNbb5dT7XRzswI2bHYH8F3JTCIabkAxcDbMQrOV3+ZEepl9zq0beUEglbX10+0Nc6bfIsBT6waPRXvWR7U4Uo7xyPnyjUFrVk3YBXQHIGwluQLeWnv7q+OBUb7kKLcUg5xznS/i/ulWA/uGHJfqjq9G7w62wAAJELQKBe1NYG3Wwyxp47li1xnsV12CfrVRwRs+6aLr8CG+2ITVfp8+J/c6/HHCbiftbKsr8DBBfCf7q1AfqVGV3iyD+H8mOqYBe9FrrTTQxv77xrzqCLW+2nhPO2igj3lOKZLMKMYOl/79T+iPKG0199uEYJFioqWykY/mn38xOvlGYwHJ/1u1mR1T92Gb3hyrrG4bHz5TmXleD7yl+rT6yIRQiAE9sbHcLVwHeM5S0Xer/6w1aKcVy2D7P6aWXGQHtuKI+7DwkAsi/iA7wHx3yNKSUGjc8Dm30w1wKafiMvtLjAWmN9uk+ZSmFHRM0Wn7DGmTpb91sG5criWuraaAAx7U1x8NCUd+TxDpduAPj5OJw/4NjrGKKG9HIrOq0+przq7cKupwhKd7+ynINuFphnhPe0T2YicWfn19CEjNn3fOBlUO0/O62XLCzizGs0hl8P0P+QEP8XLaNQgwv7xKqtS3KyC0Q9lz0P4m/Vqo6HUtYEYubtQ+gDj/i2BxX8fQW6VifqNR4Ou2zzFwF2xodq6sfWSb9qT5YCHZr204Z7sIHwfxWmdceH3sNmuDass7yTsBqzBj7Cgx+LQ0xWj16Khvu8lLBfLRO7wAAAAA==)"
      ],
      "metadata": {
        "id": "hWkXLkdWlICG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression/Individual\n",
        "##### **Name -** Rutik Retwade\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The contents of the data came from a city called Seoul. A bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort.\n",
        "\n",
        "It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes.\n",
        "\n",
        "The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information. Attribute Information: Date : year-month-day Rented Bike count - Count of bikes rented at each hour Hour - Hour of the day Temperature-Temperature in Celsius Humidity - % Wind Speed - m/s Visibility - 10m Dew point temperature - Celsius Solar radiation - MJ/m2 Rainfall - mm Snowfall - cm Seasons - Winter, Spring, Summer, Autumn Holiday - Holiday/No holiday Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)\n",
        "\n",
        "The goal of the project is to predict the number of rental bikes required at each hour for stable supply of rental bikes. Various Regression machine learning algorithms have been applied on the dataset to get the best possible prediction. Some of the key steps in the exercise involved EDA . Performed Exploratory Data Analysis on the data to gain some insights.\n",
        "\n",
        "It included univariate and multivariate analysis in which we identified certain trends, relationships, correlation and found out the features that had some impact on our dependent variable.Next we have to clean the data and perform modification . also check duplicates and missing values and outliners also removed irrelevant features .\n",
        "\n",
        "We also encoded the categorical variables. The third step was to try various machine learning algorithms on our split and standardized data. We tried different algorithms namely; Linear regression, Randomforest and XGBoost. We did hyperparameter tuning and evaluated the performance of each model using various metrics. The best performance was given by the Gradient boosting and Random forest model where the R2_score for training and test set was 0.90 and 0.84 respectively.In this exercise we can understand that  hour , temperature , wind speed, solar radiation , month, and seasons have important roles while sharing  bike . Demand of bike is higher in the morning and evening , also in summer seasons.However ,these results are not the ultimate . as this data is time dependent , the values for variables like temperature, solar_radiation, wind_speed etc., Will not always be consistent. Therefore, there will be scenarios where the model might not perform well. As machine learning is an exponentially evolving field, we will have to be prepared for all contingencies and also keep checking our model from time to time.\n",
        "\n",
        "Therefore, having a quality knowledge and keeping pace with the ever evolving ML field would surely help one to stay a step ahead in future\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D78yfJlZkKAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time as it lessens the waiting time. Eventually, providing the city with a stable supply of rental bikes becomes a major concern. The crucial part is the prediction of bike count required at each hour for the stable supply of rental bikes."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "from numpy import loadtxt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "\n",
        "from datetime import datetime     #importing for date time application\n",
        "import datetime as dt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD SEOLBIKE DATA SET FROM DRIVE\n",
        "\n",
        "bike_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Almabetter/ML Regression project/SeoulBikeData.csv',encoding ='latin')"
      ],
      "metadata": {
        "id": "o9SyEKXFeYra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "bike_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the data of bottom 5 rows to take a glimps of the data\n",
        "bike_df.tail()"
      ],
      "metadata": {
        "id": "RYZ9gtokhB7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(bike_df.shape)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting all the columns\n",
        "print(\"Features of the dataset:\")\n",
        "bike_df.columns"
      ],
      "metadata": {
        "id": "9OcWnr0OhLoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"Data is duplicated ? {bike_df.duplicated().value_counts()},unique values with {len(bike_df[bike_df.duplicated()])} duplication\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "bike_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(bike_df.isnull(), cbar=False);"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are 8760 observation and 14 features.\n",
        "* In a day we have 24 hours and we have 365 days a year so 365 multiplied by 24 = 8760, which represents the number of line in the dataset\n",
        "* There are no null values.\n",
        "* Dataset has all unique values i.e., there is no duplicate, which means data is free from bias as duplicates which can cause problems in downstream analysis, such as biasing results or making it difficult to accurately summarize the data.\n",
        "* Date has some object data types, it should be datetime data type."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(f'Features: {bike_df.columns.to_list()}')"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "bike_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breakdown of Our Features:**\n",
        "\n",
        "**Date** : *The date of the day, during 365 days from 01/12/2017 to 30/11/2018, formating in DD/MM/YYYY, type : str*, we need to convert into datetime format.\n",
        "\n",
        "**Rented Bike Count** : *Number of rented bikes per hour which our dependent variable and we need to predict that, type : int*\n",
        "\n",
        "**Hour**: *The hour of the day, starting from 0-23 it's in a digital time format, type : int, we need to convert it into category data type.*\n",
        "\n",
        "**Temperature(°C)**: *Temperature in Celsius, type : Float*\n",
        "\n",
        "**Humidity(%)**: *Humidity in the air in %, type : int*\n",
        "\n",
        "**Wind speed (m/s)** : *Speed of the wind in m/s, type : Float*\n",
        "\n",
        "**Visibility (10m)**: *Visibility in m, type : int*\n",
        "\n",
        "**Dew point temperature(°C)**: *Temperature at the beggining of the day, type : Float*\n",
        "\n",
        "**Solar Radiation (MJ/m2)**: *Sun contribution, type : Float*\n",
        "\n",
        "**Rainfall(mm)**: *Amount of raining in mm, type : Float*\n",
        "\n",
        "**Snowfall (cm)**: *Amount of snowing in cm, type : Float*\n",
        "\n",
        "**Seasons**: *Season of the year, type : str, there are only 4 season's in data *.\n",
        "\n",
        "**Holiday**: *If the day  is holiday period or not, type: str*\n",
        "\n",
        "**Functioning Day**: *If the day is a Functioning Day or not, type : str*"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in bike_df.columns.tolist():\n",
        "  print(f\"No. of unique values in {i} is {bike_df[i].nunique()}.\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing the data sets"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do we need to handle missing values?**\n",
        "* ***The real-world data often has a lot of missing values. The cause of missing values can be data corruption or failure to record data. The handling of missing data is very important during the preprocessing of the dataset as many machine learning algorithms do not support missing values.that's why we check missing values first***"
      ],
      "metadata": {
        "id": "X1qmhjVhijMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "bike_df.isnull().sum()"
      ],
      "metadata": {
        "id": "LAXbLpbBirh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "missing = pd.DataFrame((bike_df.isnull().sum())*100/bike_df.shape[0]).reset_index()\n",
        "plt.figure(figsize=(16,5))\n",
        "ax = sns.pointplot(x='index', y=0, data=missing)\n",
        "plt.xticks(rotation=90, fontsize=7)\n",
        "plt.title(\"Percentage of Missing values\")\n",
        "plt.ylabel(\"PERCENTAGE\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rsehlqp4iusf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***As we can see above there are no missing value presents thankfully***"
      ],
      "metadata": {
        "id": "-1fqP08wjXQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Checking duplicate values**"
      ],
      "metadata": {
        "id": "Hp7mQunzjnSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is it important to remove duplicate records from my data?**\n",
        "* **\"Duplication\" just means that you have repeated data in your dataset. This could be due to things like data entry errors or data collection methods. By removing duplication in our data set,  Time and money are saved by not sending identical communications multiple times to the same person.**"
      ],
      "metadata": {
        "id": "hGFexzplj6Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Duplicate Values\n",
        "value=len(bike_df[bike_df.duplicated()])\n",
        "print(\"The number of duplicate values in the data set is = \",value)"
      ],
      "metadata": {
        "id": "Bc4e061Uj_S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***In the above data after count the missing and duplicate value we came to know that there are no missing and duplicate value present.***\n",
        "* ***Some of  the columns name in the dataset are too large and clumsy so we change them into some simple name, and it don't affect our end results.***"
      ],
      "metadata": {
        "id": "u2f8jcSfkBeZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Changing column name**"
      ],
      "metadata": {
        "id": "uEY6IYrwkWbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename the complex columns name\n",
        "bike_df=bike_df.rename(columns={'Rented Bike Count':'Rented_Bike_Count',\n",
        "                                'Temperature(°C)':'Temperature',\n",
        "                                'Humidity(%)':'Humidity',\n",
        "                                'Wind speed (m/s)':'Wind_speed',\n",
        "                                'Visibility (10m)':'Visibility',\n",
        "                                'Dew point temperature(°C)':'Dew_point_temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
        "                                'Rainfall(mm)':'Rainfall',\n",
        "                                'Snowfall (cm)':'Snowfall',\n",
        "                                'Functioning Day':'Functioning_Day'})"
      ],
      "metadata": {
        "id": "Ye2nayO_khvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***Python read \"Date\" column as a object type basically it reads as a string, as the date column is very important to analyze the users behaviour so we need to convert it into datetime format then we split it into 3 column i.e 'year', 'month', 'day'as a category data type.***\n"
      ],
      "metadata": {
        "id": "dO9-KcTckqmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Breaking date column**"
      ],
      "metadata": {
        "id": "xSqHNWI5ky2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the \"Date\" column into three \"year\",\"month\",\"day\" column\n",
        "bike_df['Date'] = bike_df['Date'].str.replace('-', '/')\n",
        "bike_df['Date'] = bike_df['Date'].apply(lambda x: dt.datetime.strptime(x, \"%d/%m/%Y\"))"
      ],
      "metadata": {
        "id": "vK44UjPlk-kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['year'] = bike_df['Date'].dt.year\n",
        "bike_df['month'] = bike_df['Date'].dt.month\n",
        "bike_df['day'] = bike_df['Date'].dt.day_name()"
      ],
      "metadata": {
        "id": "SSHe63WzlCG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
        "bike_df['weekdays_weekend']=bike_df['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
        "bike_df=bike_df.drop(columns=['Date','day','year'],axis=1)"
      ],
      "metadata": {
        "id": "lEXWpqL3lEnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***So we convert the \"date\" column into 3 different column i.e \"year\",\"month\",\"day\".***\n",
        "* ***The \"year\" column in our data set is basically contain the 2 unique number contains the details of from 2017 december to 2018 november so if i consider this is a one year then we don't need the \"year\" column so we drop it***.\n",
        "* ***The other column \"day\", it contains the details about the each day of the month, for our relevence we don't need each day of each month data but we need the data about, if a day is a weekday or a weekend so we convert it into this format and drop the \"day\" column***."
      ],
      "metadata": {
        "id": "V45LTEoClK6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.head()"
      ],
      "metadata": {
        "id": "TyHPQ9BulO_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['weekdays_weekend'].value_counts()"
      ],
      "metadata": {
        "id": "E8SfHhKGlZWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Changing data types**"
      ],
      "metadata": {
        "id": "4dDLNpRilcOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***As \"Hour\",\"month\",\"weekdays_weekend\" column are show as a integer data type but actually it is a category data tyepe. so we need to change this data tyepe if we not then, while doing the further anlysis and correleted with this then the values are not actually true so we can mislead by this.***"
      ],
      "metadata": {
        "id": "Lrcfgy38lnmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the int64 column into catagory column\n",
        "cols=['Hour','month','weekdays_weekend']\n",
        "for col in cols:\n",
        "  bike_df[col]=bike_df[col].astype('category')"
      ],
      "metadata": {
        "id": "BRWVwUUBls2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#let's check the result of data type\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "dLGWFzkglu0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.columns"
      ],
      "metadata": {
        "id": "2Sd8X3QUlw3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Exploratory Data Analysis Of The Data Set**"
      ],
      "metadata": {
        "id": "NI-crgE_o_HD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do we perform EDA?**\n",
        "* ***An EDA is a thorough examination meant to uncover the underlying structure of a data set and is important for a company because it exposes trends, patterns, and relationships that are not readily apparent.***"
      ],
      "metadata": {
        "id": "vv69ZHF_o5vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Univariate analysis**"
      ],
      "metadata": {
        "id": "RoOF0IwPpI3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do you do univariate analysis?**\n",
        "* ***The key objective of Univariate analysis is to simply describe the data to find patterns within the data.***"
      ],
      "metadata": {
        "id": "Q5hr2dd8pS3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analysis on dependent variable**"
      ],
      "metadata": {
        "id": "nDnR_mqHpdhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a dependent variable in data analysis?**\n",
        "* ***we analyse our dependent variable,A dependent variable is a variable whose value will change depending on the value of another variable.***"
      ],
      "metadata": {
        "id": "INUkL4hOpWTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Analysis on catogorical variable**"
      ],
      "metadata": {
        "id": "m1O1xosbpfGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***Our dependent variable is \"Rented Bike Count\" so we need to analysis this column with the other columns by using some visualisation plot.first we analyze the category data type then we proceed with the numerical data type***"
      ],
      "metadata": {
        "id": "MnqGarW-pZUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Month**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,7))\n",
        "sns.barplot(data=bike_df,x='month',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Month')"
      ],
      "metadata": {
        "id": "2V-VV7cEqZ0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used bar plot for above analysis because of bar graph we can easily get insight from the data."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above bar plot we can clearly say that, from the month 5 to 10\n",
        " (May - October) the demand of the rented bike is high as compare to other months.These months came inside the summer season."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Weekend_weekdays"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(8,6))\n",
        "sns.barplot(data=bike_df,x='weekdays_weekend',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to weekdays_weekenday ')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,7))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='weekdays_weekend',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to weekdays_weekend ')"
      ],
      "metadata": {
        "id": "qG_BbxQZusDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used here bar plot because it will provide us information about bike rented on weekdays and weekends"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above point plot and bar plot we can say that, in the week days which represent in blue colur show that the demand of the bike higher because of the office.\n",
        "Peak Time are 7 am to 9 am and 5 pm to 7 pm\n",
        "The orange colur represent the weekend days, and it show that the demand of rented bikes are very low specially in the morning hour but when the evening start from 4 pm to 8 pm the demand slightly increases.  "
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hours**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,7))\n",
        "sns.barplot(data=bike_df,x='Hour',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Hour ')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above plot which shows, the use of rented bike according the hours and the data are from all over the year.\n",
        "\n",
        "generally people use rented bikes during their working hour from 7am to 9am and 5pm to 7pm."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Functioning day**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualization code\n",
        "fig,ax=plt.subplots(figsize=(8,6))\n",
        "sns.barplot(data=bike_df,x='Functioning_Day',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Functioning Day ')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,7))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='Functioning_Day',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to Functioning Day ')"
      ],
      "metadata": {
        "id": "mFL20IjJvgz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***In the above bar plot and point plot which shows the use of rented bike in functioning day or non functioning day, and it clearly shows that,***\n",
        "* ***Peoples don't use reneted bikes in no functioning day.***"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Seasons**"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,6))\n",
        "sns.barplot(data=bike_df,x='Seasons',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Seasons ')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,6))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='Seasons',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to seasons ')"
      ],
      "metadata": {
        "id": "IEIPzXXGv6Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***In the above bar plot and point plot which shows, the use of rented bike in four different seasons, and it clearly shows that,***\n",
        "* ***In summer season the use of rented bike is high and peak time is 7am-9am and 5pm-7pm.***\n",
        "* ***In winter season the use of rented bike is very low maybe because of snowfall, fog, cold etc.***"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Holiday**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(8,6))\n",
        "sns.barplot(data=bike_df,x='Holiday',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Holiday ')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(12,6))\n",
        "sns.pointplot(data=bike_df,x='Hour',y='Rented_Bike_Count',hue='Holiday',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to Holiday ')"
      ],
      "metadata": {
        "id": "P6pQXwxTwRPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***In the above bar plot and point plot which shows the use of rented bike in a holiday, and it clearly shows that,***\n",
        "* ***In holiday, people uses the rented bike from 2pm-8pm***"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Analyze numerical variable**"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Numerical Data**\n",
        "\n",
        "* ***Numerical data is a data type expressed in numbers, rather than natural language description. Sometimes called quantitative data, numerical data is always collected in number form. Numerical data differentiates itself from other number form data types with its ability to carry out arithmetic operations with these numbers.***"
      ],
      "metadata": {
        "id": "j50uJvrv-vFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<b> Pays little attention to the skewness of our numerical features"
      ],
      "metadata": {
        "id": "VMiqV4FN-30T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate numerical features from the dataframe\n",
        "numeric_features= bike_df.select_dtypes(exclude=['object','category'])\n",
        "numeric_features"
      ],
      "metadata": {
        "id": "uZvxofe2-7T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing displots to analyze the distribution of all numerical features\n",
        "\n",
        "n=1\n",
        "plt.figure(figsize=(15,10))\n",
        "for i in numeric_features.columns:\n",
        "  plt.subplot(3,3,n)\n",
        "  n=n+1\n",
        "  sns.distplot(bike_df[i])\n",
        "  plt.title(i)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "ZZiJfvgVACh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Right skewed columns are\n",
        "Rented Bike Count (Its also our Dependent variable),\n",
        "Wind speed (m/s),\n",
        "Solar Radiation (MJ/m2),\n",
        "Rainfall(mm),\n",
        "Snowfall (cm),\n",
        "\n",
        "## Left skewed columns are\n",
        "Visibility (10m),\n",
        "Dew point temperature(°C)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<b> Lets try to find how is the relation of numerical features with our dependent variable"
      ],
      "metadata": {
        "id": "F150OXaSAZeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Numerical VS Rented bike Count**"
      ],
      "metadata": {
        "id": "5v-X4vu2AfZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Temperature\"\n",
        "bike_df.groupby('Temperature').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "ZLpzpKVqAvpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***From the above plot we see that, people like to ride bikes when it is pretty hot around 25°C in average***"
      ],
      "metadata": {
        "id": "BDLfaCBWAzwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Dew_point_temperature\"\n",
        "bike_df.groupby('Dew_point_temperature').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "40iHZQ3RA23z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***From the above plot of \"Dew_point_temperature', is almost same as the 'temperature' there is some similarity present we can check it in our next step.***"
      ],
      "metadata": {
        "id": "MIYG_7PdA885"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Solar_Radiation\"\n",
        "bike_df.groupby('Solar_Radiation').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "Pl9LBrcJBAIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***from the above plot we see that, the amount of rented bikes is huge, when there is solar radiation, the count of rents is around 1000***"
      ],
      "metadata": {
        "id": "79dGP-0GBC9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Snowfall\"\n",
        "bike_df.groupby('Snowfall').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "aPDEDyYYBIAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***We can see from the plot that, on the y-axis, the amount of rented bike is very low. When we have more than 4 cm of snow, the bike rents is much lower***"
      ],
      "metadata": {
        "id": "iktUiHiTBRp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Rainfall\"\n",
        "bike_df.groupby('Rainfall').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "oSs2dyjRBUvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***We can see from the above plot that, even if it rains a lot the demand of of rent bikes is not decreasing, here for example even if we have 20 mm of rain there is a big peak of rented bikes***"
      ],
      "metadata": {
        "id": "qTnIpuRBBaIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Wind_speed\"\n",
        "bike_df.groupby('Wind_speed').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "qP9MZS5EBdde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***We can see from the above plot that, the demand of rented bike is uniformly distribute despite of wind speed but when the speed of wind is 7 m/s then the demand of bike also increase that clearly means people love to ride bikes when its little windy.***"
      ],
      "metadata": {
        "id": "4iG1dmg9BjMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Regression Plot**"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***The regression plots in seaborn are primarily intended to add a visual guide that helps to emphasize patterns in a dataset during exploratory data analyses. Regression plots as the name suggests creates a regression line between 2 parameters and helps to visualize their linear relationships.***"
      ],
      "metadata": {
        "id": "-aZv31eyBuyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#printing the regression plot for all the numerical features\n",
        "for col in numeric_features:\n",
        "  fig,ax=plt.subplots(figsize=(8,4))\n",
        "  sns.regplot(x=bike_df[col],y=bike_df['Rented_Bike_Count'],scatter_kws={\"color\": 'orange'}, line_kws={\"color\": \"black\"})"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***From the above regression plot of all numerical features we see that the columns  'Temperature', 'Wind_speed','Visibility', 'Dew_point_temperature', 'Solar_Radiation' are positively relation to the target variable.***\n",
        "\n",
        "\n",
        "* ***which means the rented bike count increases with increase of these features.***\n",
        "* ***'Rainfall','Snowfall','Humidity' these features are negatively related with the target variaable which means the rented bike count decreases when these features increase.***"
      ],
      "metadata": {
        "id": "_9zK_vMXCBLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Normalise Rented_Bike_Count column data**"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***The data normalization (also referred to as data pre-processing) is a basic element of data mining. It means transforming the data, namely converting the source data in to another format that allows processing data effectively. The main purpose of data normalization is to minimize or even exclude duplicated data***"
      ],
      "metadata": {
        "id": "NMUcy3HHE5Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution plot of Rented Bike Count\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.xlabel('Rented_Bike_Count')\n",
        "plt.ylabel('Density')\n",
        "ax=sns.distplot(bike_df['Rented_Bike_Count'],hist=True ,color=\"y\")\n",
        "ax.axvline(bike_df['Rented_Bike_Count'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(bike_df['Rented_Bike_Count'].median(), color='black', linestyle='dashed', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***The above graph shows that, Rented Bike Count has moderate right skewness. Since the assumption of linear regression is that 'the distribution of dependent variable has to be normal', so we should perform some operation to make it normal.***"
      ],
      "metadata": {
        "id": "sk_5Qre7FCGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding Outliers and treatment**"
      ],
      "metadata": {
        "id": "wsVq4FroFGHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot for Rented bike Count to check outliers\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "sns.boxplot(x=bike_df['Rented_Bike_Count'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FmvHQVkgFN6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outliers treatments\n",
        "bike_df.loc[bike_df['Rainfall']>=4,'Rainfall']= 4\n",
        "bike_df.loc[bike_df['Solar_Radiation']>=2.5,'Solar_Radiation']=2.5\n",
        "bike_df.loc[bike_df['Snowfall']>2,'Snowfall']= 2\n",
        "bike_df.loc[bike_df['Wind_speed']>=4,'Wind_speed']= 4"
      ],
      "metadata": {
        "id": "WbhQ3XvwFVw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have applied outlier treatment techniques to the dataset by replacing the outliers with the maximum values."
      ],
      "metadata": {
        "id": "AUpUAygsFZsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying square root to Rented Bike Count to improve skewness\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "ax=sns.distplot(np.sqrt(bike_df['Rented_Bike_Count']), color=\"y\")\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(np.sqrt(bike_df['Rented_Bike_Count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b4tkSQJDFd6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***Since we have generic rule of applying Square root for the skewed variable in order to make it normal .After applying Square root to the skewed Rented Bike Count, here we get almost normal distribution.***"
      ],
      "metadata": {
        "id": "7EUlzPsQFnOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#After applying sqrt on Rented Bike Count check wheater we still have outliers\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "sns.boxplot(x=np.sqrt(bike_df['Rented_Bike_Count']))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VgEJV_6FFrII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***After applying Square root to the Rented Bike Count column, we find that there is no outliers present.***"
      ],
      "metadata": {
        "id": "0fD_OvJhFzWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.corr()"
      ],
      "metadata": {
        "id": "BBGRRzITF1um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Checking of Correlation between variables**"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Checking in OLS Model"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable**"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the module\n",
        "#assign the 'x','y' value\n",
        "import statsmodels.api as sm\n",
        "X = bike_df[[ 'Temperature','Humidity',\n",
        "       'Wind_speed', 'Visibility','Dew_point_temperature',\n",
        "       'Solar_Radiation', 'Rainfall', 'Snowfall']]\n",
        "Y = bike_df['Rented_Bike_Count']\n",
        "bike_df.head()"
      ],
      "metadata": {
        "id": "3cQLkltnGarp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add a constant column\n",
        "X = sm.add_constant(X)\n",
        "X"
      ],
      "metadata": {
        "id": "d1PsCwxDGgDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## fit an OLS model\n",
        "model= sm.OLS(Y, X).fit()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "oqqqYYNsGlet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **R sqauare and Adj Square are near to each other. 40% of variance in the Rented Bike count is  explained by the model.**\n",
        "*  **For F statistic , P value is less than 0.05 for 5% levelof significance.**\n",
        "*  **P value of dew point temp and visibility are very high and they are not significant.**\n",
        "\n",
        "*  **Omnibus tests the skewness and kurtosis of the residuals. Here the value of Omnibus is high., it shows we have skewness in our data.**\n",
        "*  **The condition number is large, 3.11e+04. This might indicate that there are strong multicollinearity or other numerical problems**   \n",
        "*  **Durbin-Watson tests for autocorrelation of the residuals. Here value is less than 0.5. We can say that there exists a positive auto correlation among the variables.**"
      ],
      "metadata": {
        "id": "WhcBxB7qHP7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.corr()"
      ],
      "metadata": {
        "id": "pCsakPTcHSYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***From the OLS model we find that the 'Temperature' and  'Dew_point_temperature' are highly correlated so we need to drop one of them.***\n",
        "* ***For droping them we check the (P>|t|) value from above table and we can see that the 'Dew_point_temperature' value is higher so we need to drop Dew_point_temperature column***\n",
        "* ***For clarity, we use visualisation i.e heatmap in next step***"
      ],
      "metadata": {
        "id": "ymcsQWZIHbxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Heat Map**"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **A correlation Heatmap is a type of graphical representation that displays the correlation matrix, which helps to determine the correlation between different variables.**"
      ],
      "metadata": {
        "id": "w4ETjDgLHkU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking correlation using heatmap\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(bike_df.corr(),cmap='PiYG',annot=True)"
      ],
      "metadata": {
        "id": "Rp-nw1qdHxj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can observe on the heatmap that on the target variable line, the most positively correlated variables to the rent are**:\n",
        "* the temperature\n",
        "* the dew point temperature\n",
        "* the solar radiation"
      ],
      "metadata": {
        "id": "dGkgTrJDH6rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**And most negatively correlated variables are**:\n",
        "* humidity\n",
        "* rainfall"
      ],
      "metadata": {
        "id": "OsLbNJstIAOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ***From the above correlation heatmap, We see that there is a positive\n",
        "correlation between columns 'Temperature' and 'Dew point temperature' i.e 0.91 so even if we drop this column then it won't affect the outcome of our analysis. And they have the same variations, so we can drop the column 'Dew point temperature(°C)'.***"
      ],
      "metadata": {
        "id": "MqsdEailIIML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the Dew point temperature column\n",
        "bike_df=bike_df.drop(['Dew_point_temperature'],axis=1)\n"
      ],
      "metadata": {
        "id": "oFcqBwmlIM8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.info()"
      ],
      "metadata": {
        "id": "fWVBGFuEIdK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Engineering & Data Pre-processing**"
      ],
      "metadata": {
        "id": "KEzrNMXTJLyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the dummy variables"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A dataset may contain various type of values, sometimes it consists of categorical values. So, in-order to use those categorical value for programming efficiently we create dummy variables.**"
      ],
      "metadata": {
        "id": "zzthPlz7Ip0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign all categorical features to a variable\n",
        "categorical_features=list(bike_df.select_dtypes(['object','category']).columns)\n",
        "categorical_features=pd.Index(categorical_features)\n",
        "categorical_features"
      ],
      "metadata": {
        "id": "KSXeRbbNJPhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###One Hot Encoding"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A one hot encoding allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical.**"
      ],
      "metadata": {
        "id": "xYzNE72XJFDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creat a copy\n",
        "bike_df_copy = bike_df\n",
        "\n",
        "def one_hot_encoding(data, column):\n",
        "    data = pd.concat([data, pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)\n",
        "    data = data.drop([column], axis=1)\n",
        "    return data\n",
        "\n",
        "for col in categorical_features:\n",
        "    bike_df_copy = one_hot_encoding(bike_df_copy, col)\n",
        "bike_df_copy.head()"
      ],
      "metadata": {
        "id": "ph6mOHJpJUrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model training**"
      ],
      "metadata": {
        "id": "eTFMRBabOMcz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Train Test split for regression**"
      ],
      "metadata": {
        "id": "J1gJimRzPmGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before, fitting any model it is a rule of thumb to split the dataset into a training and test set. This means some proportions of the data will go into training the model and some portion will be used to evaluate how our model is performing on any unseen data. The proportions may vary from 60:40, 70:30, 75:25 depending on the person but mostly used is 80:20 for training and testing respectively. In this step we will split our data into training and testing set using scikit learn library."
      ],
      "metadata": {
        "id": "wyyAHUr4PqnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign the value in X and Y\n",
        "X = bike_df_copy.drop(columns=['Rented_Bike_Count'], axis=1)\n",
        "y = np.sqrt(bike_df_copy['Rented_Bike_Count'])"
      ],
      "metadata": {
        "id": "SXBleGcQP23W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "67iNlR7XP5s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "y6BkmLLGP_Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create test and train data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "dIQYCvsdQF5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df_copy.info()"
      ],
      "metadata": {
        "id": "rowLyGZlQJqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df_copy.describe().columns"
      ],
      "metadata": {
        "id": "Fw42P8hDQMqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The mean squared error (MSE) tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them.\n",
        "It’s called the mean squared error as you’re finding the average of a set of errors. The lower the MSE, the better the forecast.\n",
        "\n",
        "* MSE formula = (1/n) * Σ(actual – forecast)2\n",
        "Where:\n",
        "\n",
        "*   n = number of items,\n",
        "* Σ = summation notation,\n",
        "* Actual = original or observed y-value,\n",
        "* Forecast = y-value from regression.\n",
        "\n",
        "* Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors).\n",
        "\n",
        "* Mean Absolute Error (MAE) are metrics used to evaluate a Regression Model. ... Here, errors are the differences between the predicted values (values predicted by our regression model) and the actual values of a variable.\n",
        "\n",
        "* R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n",
        "\n",
        "* Formula for R-Squared\n",
        "\\begin{aligned} &\\text{R}^2 = 1 - \\frac{ \\text{Unexplained Variation} }{ \\text{Total Variation} } \\\\ \\end{aligned}\n",
        "​\n",
        "  \n",
        "* R\n",
        "2\n",
        " =1−\n",
        "Total Variation\n",
        "Unexplained Variation\n",
        "​\n",
        "\n",
        "* Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
        "​\n"
      ],
      "metadata": {
        "id": "a97EzSqTQRNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LINEAR REGRESSION"
      ],
      "metadata": {
        "id": "AS5-HnvoQWGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression models describe the relationship between variables by fitting a line to the observed data. Linear regression models use a straight line"
      ],
      "metadata": {
        "id": "bTUJ6QrZQZ_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression uses a linear approach to model the relationship between independent and dependent variables. In simple words its a best fit line drawn over the values of independent variables and dependent variable. In case of single variable, the formula is same as straight line equation having an intercept and slope.\n",
        "\n",
        "$$ \\text{y_pred} = \\beta_0 + \\beta_1x$$\n",
        "\n",
        "where $$\\beta_0 \\text{ and } \\beta_1$$ are intercept and slope respectively.\n",
        "\n",
        "In case of multiple features the formula translates into:\n",
        "\n",
        "$$ \\text{y_pred} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 +\\beta_3x_3 +.....$$\n",
        "\n",
        "where x_1,x_2,x_3 are the features values and\n",
        "$$\\beta_0,\\beta_1,\\beta_2.....$$\n",
        " are weights assigned to each of the features. These become the parameters which the algorithm tries to learn using Gradient descent.\n",
        "\n",
        "Gradient descent is the process by which the algorithm tries to update the parameters using  a loss function . Loss function is nothing but the diffence between the actual values and predicted values(aka error or residuals). There are different types of loss function but this is the simplest one. Loss function summed over all observation gives the cost functions. The role of gradient descent is to update the parameters till the cost function is minimized i.e., a global minima is reached. It uses a hyperparameter 'alpha' that gives a weightage to the cost function and decides on how big the steps to take. Alpha is called as the learning rate. It is always necesarry to keep an optimal value of alpha as high and low values of alpha might make the gradient descent overshoot or get stuck at a local minima. There are also some basic assumptions that must be fulfilled before implementing this algorithm. They are:\n",
        "\n",
        "1. No multicollinearity in the dataset.\n",
        "\n",
        "2. Independent variables should show linear relationship with dv.\n",
        "\n",
        "3. Residual mean should be 0 or close to 0.\n",
        "\n",
        "4. There should be no heteroscedasticity i.e., variance should be constant along the line of best fit.\n",
        "\n",
        "\n",
        "\n",
        "Let us now implement our first model.\n",
        "We will be using LinearRegression from scikit library.\n"
      ],
      "metadata": {
        "id": "hZm_nco_QdoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg= LinearRegression().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "lZ117WVVQgez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "reg.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "i63_Us_AQiWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the coefficeint\n",
        "reg.coef_"
      ],
      "metadata": {
        "id": "vJu3REGzQke1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train=reg.predict(X_train)\n",
        "y_pred_test=reg.predict(X_test)"
      ],
      "metadata": {
        "id": "ZbmgSV-WQnxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error((y_train), (y_pred_train))\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_train, y_pred_train)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score(y_train, y_pred_train)\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "POfejtBkQqqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our train set's r2 score value is 0.79 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "dCJq9XbuQwWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "JHtfhoRNQyW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error(y_test, y_pred_test)\n",
        "print(\"MSE :\",MSE_lr)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_test, y_pred_test)\n",
        "print(\"MAE :\",MAE_lr)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score((y_test), (y_pred_test))\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score((y_test), (y_pred_test)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",Adjusted_R2_lr )\n"
      ],
      "metadata": {
        "id": "9wFwW7b6Q18I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The test set's r2_score is 0.80. This means our linear model is\n",
        "performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "a4ssxEBIRGlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "test_df=pd.DataFrame(dict2,index=[1])"
      ],
      "metadata": {
        "id": "SA_TGc_eRIht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Heteroscedasticity**\n",
        "\n",
        "\n",
        "Heteroscedasticity refers to a situation where the variance of the errors (residuals) is not constant across all levels of the independent variable(s) in a regression model.This violates one of the assumptions of linear regression, which is that the variance of the errors should be constant (homoscedastic) for all levels of the independent variable(s). If the plot shows a funnel shape, with the spread of residuals increasing or decreasing as the predicted values increase, this is an indication of heteroscedasticity."
      ],
      "metadata": {
        "id": "92ZAHpfERLuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity - Residual plot\n",
        "plt.scatter((y_pred_test),(y_test)-(y_pred_test))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D06DfV0jRgpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual Price vs predicte for Linear Regression plot\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(y_pred_test)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8D4A42bkRkWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ridge and Lasso Regression**\n",
        "\n",
        "* Ridge and Lasso Regression are types of Regularization techniques\n",
        "* Regularization techniques are used to deal with overfitting and when the dataset is large\n",
        "* Ridge and Lasso Regression involve adding penalties to the regression function"
      ],
      "metadata": {
        "id": "3g-EhVY6Ron3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lasso regression**"
      ],
      "metadata": {
        "id": "oDVnqzThUCE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso regression analysis is a shrinkage and variable selection method for linear regression models. The goal of lasso regression is to obtain the subset of predictors that minimizes prediction error for a quantitative response variable. It uses the Linear regression model with L1 regularization."
      ],
      "metadata": {
        "id": "3QwUdiO_UMOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "ATJwhzgcUQIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of Lasso Regression implementation\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso = Lasso(alpha=1.0, max_iter=3000)\n",
        "# Fit the Lasso model\n",
        "lasso.fit(X_train, y_train)\n",
        "# Create the model score\n",
        "print(lasso.score(X_test, y_test), lasso.score(X_train, y_train))"
      ],
      "metadata": {
        "id": "uJFgwR3-UVBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_lasso=lasso.predict(X_train)\n",
        "y_pred_test_lasso=lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "WeiVahH2UYp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_l= mean_squared_error((y_train), (y_pred_train_lasso))\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_train, y_pred_train_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score(y_train, y_pred_train_lasso)\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "npKJkH30Uc_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like train set's r2 score value is 0.39 that means our model is not able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "21pu1NyCUiBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_l= mean_squared_error(y_test, y_pred_test_lasso)\n",
        "print(\"MSE :\",MSE_l)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_test, y_pred_test_lasso)\n",
        "print(\"MAE :\",MAE_l)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score((y_test), (y_pred_test_lasso))\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l=(1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "wOfbDoQ9Ul97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The test set's r2_score is 0.38. This means our linear model is  not performing well on the data.**"
      ],
      "metadata": {
        "id": "VpQP_SNQUscD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "wpxptKNlUwyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity- Residual plot\n",
        "plt.scatter((y_pred_test_lasso),(y_test-y_pred_test_lasso))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wKlP2qchU0SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(np.array(y_pred_test_lasso))\n",
        "plt.plot(np.array((y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kIcykkEkU4Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ridge regression**"
      ],
      "metadata": {
        "id": "LW1jNSkSU92y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridge regression is a method of estimating the coefficients of regression models in scenarios where the independent variables are highly correlated. It uses the linear regression model with the L2 regularization method."
      ],
      "metadata": {
        "id": "IcNz8PQkVGIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge= Ridge(alpha=0.1)"
      ],
      "metadata": {
        "id": "znRKf77PVJYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "ridge.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "pgEqycA8VLq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "ridge.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "rlzsiAMEVOLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_ridge=ridge.predict(X_train)\n",
        "y_pred_test_ridge=ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "Lm3UTJz6VR2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_ridge"
      ],
      "metadata": {
        "id": "zNTgDsoLVUT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test_ridge"
      ],
      "metadata": {
        "id": "q3hrmInNVVgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_r= mean_squared_error((y_train), (y_pred_train_ridge))\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_train, y_pred_train_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score(y_train, y_pred_train_ridge)\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "uSN1-LraVaNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our train set's r2 score value is 0.79 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "LuVRZQl5WDkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "7MMesofjWGUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_r= mean_squared_error(y_test, y_pred_test_ridge)\n",
        "print(\"MSE :\",MSE_r)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_test, y_pred_test_ridge)\n",
        "print(\"MAE :\",MAE_r)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score((y_test), (y_pred_test_ridge))\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "OyzNk6wDWJvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.80. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "rwWGxkDMWRek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "08r-e-qQWTXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity - Residual plot\n",
        "plt.scatter((y_pred_test_ridge),(y_test)-(y_pred_test_ridge))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "plJVqpoCWVSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot((y_pred_test_ridge))\n",
        "plt.plot((np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6OEQobEdWYyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Elastic net regression**"
      ],
      "metadata": {
        "id": "DfZY8Ri8WdUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elastic Net regression is a linear regression model that combines both L1 (Lasso) and L2 (Ridge) regularization penalties to overcome some of the limitations of each individual method.\n",
        "\n",
        "The model introduces two hyperparameters, alpha and l1_ratio, which control the strength of the L1 and L2 penalties, respectively.\n",
        "Elastic Net regression is particularly useful when dealing with datasets that have high dimensionality and multicollinearity between features."
      ],
      "metadata": {
        "id": "ydn_U6ApWmFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.linear_model import ElasticNet\n",
        "#a * L1 + b * L2\n",
        "#alpha = a + b and l1_ratio = a / (a + b)\n",
        "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)"
      ],
      "metadata": {
        "id": "a15smFSiWoUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "elasticnet.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "Nku20AHaWqfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "elasticnet.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "kLsS1rPvWsFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_en=elasticnet.predict(X_train)\n",
        "y_pred_test_en=elasticnet.predict(X_test)"
      ],
      "metadata": {
        "id": "YiW5WMsXWuQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred_train_en)\n",
        "print(y_pred_test_en)"
      ],
      "metadata": {
        "id": "wcg4M0HlWwTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_e= mean_squared_error((y_train), (y_pred_train_en))\n",
        "print(\"MSE :\",MSE_e)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_train, y_pred_train_en)\n",
        "print(\"MAE :\",MAE_e)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score(y_train, y_pred_train_en)\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "sINVQhyvWzFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our train set's r2 score value is 0.64 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "UixsXCheW4vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Elastic net regression ',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "8WaOKwIoXEGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_e= mean_squared_error(y_test, y_pred_test_en)\n",
        "print(\"MSE :\",MSE_e)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_test, y_pred_test_en)\n",
        "print(\"MAE :\",MAE_e)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score((y_test), (y_pred_test_en))\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "292eZOJDXGr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.63. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n"
      ],
      "metadata": {
        "id": "CKsl3GPqXKb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Elastic net regression Test',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "AeyCvY5pXMrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity- Residual plo\n",
        "plt.scatter((y_pred_test_en),(y_test)-(y_pred_test_en))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-7E-w09DXPWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(np.array(y_pred_test_en))\n",
        "plt.plot((np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GJU15Vi2XSeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DECISION TREE**"
      ],
      "metadata": {
        "id": "Tmk3fj8BYp_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision tree is a type of supervised machine learning algorithm that is commonly used for classification and regression tasks. It works by recursively splitting the data into subsets based on the values of certain attributes, ultimately arriving at a set of decision rules that can be used to classify or predict outcomes for new data."
      ],
      "metadata": {
        "id": "FApL5doHYvPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "decision_regressor = DecisionTreeRegressor(criterion='friedman_mse', max_depth=8,\n",
        "                      max_features=9, max_leaf_nodes=100,)\n",
        "decision_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "T8WXMRF-Yy5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_d = decision_regressor.predict(X_train)\n",
        "y_pred_test_d = decision_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "7Ugp3qmYY1mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred_train_d)\n",
        "print(y_pred_test_d)"
      ],
      "metadata": {
        "id": "3dnZfNHOY4HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",decision_regressor.score(X_train,y_train))\n",
        "\n",
        "#calculate MSE\n",
        "MSE_d= mean_squared_error(y_train, y_pred_train_d)\n",
        "print(\"MSE :\",MSE_d)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_d=np.sqrt(MSE_d)\n",
        "print(\"RMSE :\",RMSE_d)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_d= mean_absolute_error(y_train, y_pred_train_d)\n",
        "print(\"MAE :\",MAE_d)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_d= r2_score(y_train, y_pred_train_d)\n",
        "print(\"R2 :\",r2_d)\n",
        "Adjusted_R2_d=(1-(1-r2_score(y_train, y_pred_train_d))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_d))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "UmM8IPtsY6xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our train set's r2 score value is 0.70, that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "sJVFL_ljY_z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Dicision tree regression ',\n",
        "       'MAE':round((MAE_d),3),\n",
        "       'MSE':round((MSE_d),3),\n",
        "       'RMSE':round((RMSE_d),3),\n",
        "       'R2_score':round((r2_d),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_d),2)\n",
        "      }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "GBlvYGyRZB-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_d= mean_squared_error(y_test, y_pred_test_d)\n",
        "print(\"MSE :\",MSE_d)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_d=np.sqrt(MSE_d)\n",
        "print(\"RMSE :\",RMSE_d)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_d= mean_absolute_error(y_test, y_pred_test_d)\n",
        "print(\"MAE :\",MAE_d)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_d= r2_score((y_test), (y_pred_test_d))\n",
        "print(\"R2 :\",r2_d)\n",
        "Adjusted_R2_d=(1-(1-r2_score((y_test), (y_pred_test_d)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_d)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "TsnjdkL5ZGAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.68. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "A0bokqGKZLRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Dicision tree regression ',\n",
        "       'MAE':round((MAE_d),3),\n",
        "       'MSE':round((MSE_d),3),\n",
        "       'RMSE':round((RMSE_d),3),\n",
        "       'R2_score':round((r2_d),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_d),2)\n",
        "      }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "3c-CqUrXZOiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity - Residual plot\n",
        "plt.scatter((y_pred_test_d),(y_test)-(y_pred_test_d))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mnr7xKw4ZQib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot((np.array(y_pred_test_d)))\n",
        "plt.plot(np.array((y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SagEMd45ZTQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RANDOM FOREST**"
      ],
      "metadata": {
        "id": "maUbiossZXVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Create an instance of the RandomForestRegressor\n",
        "rf_model = RandomForestRegressor()\n",
        "\n",
        "rf_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "oHS_E69zZbpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_r = rf_model.predict(X_train)\n",
        "y_pred_test_r = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "eC3z1q3dZgLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",rf_model.score(X_train,y_train))\n",
        "\n",
        "#calculate MSE\n",
        "MSE_rf= mean_squared_error(y_train, y_pred_train_r)\n",
        "print(\"MSE :\",MSE_rf)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_rf=np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\",RMSE_rf)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_rf= mean_absolute_error(y_train, y_pred_train_r)\n",
        "print(\"MAE :\",MAE_rf)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_rf= r2_score(y_train, y_pred_train_r)\n",
        "print(\"R2 :\",r2_rf)\n",
        "Adjusted_R2_rf=(1-(1-r2_score(y_train, y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_r))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "HvT8Z99sZjxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our train set's r2 score value is 0.98 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "BgwxZNnxZokq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Random forest regression ',\n",
        "       'MAE':round((MAE_rf),3),\n",
        "       'MSE':round((MSE_rf),3),\n",
        "       'RMSE':round((RMSE_rf),3),\n",
        "       'R2_score':round((r2_rf),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_rf ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "E457xNgPZrl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_rf= mean_squared_error(y_test, y_pred_test_r)\n",
        "print(\"MSE :\",MSE_rf)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_rf=np.sqrt(MSE_rf)\n",
        "print(\"RMSE :\",RMSE_rf)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_rf= mean_absolute_error(y_test, y_pred_test_r)\n",
        "print(\"MAE :\",MAE_rf)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_rf= r2_score((y_test), (y_pred_test_r))\n",
        "print(\"R2 :\",r2_rf)\n",
        "Adjusted_R2_rf=(1-(1-r2_score((y_test), (y_pred_test_r)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_r)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "VE5swUCPZuTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.91. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "oRuAu5L1Zxpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Random forest regression ',\n",
        "       'MAE':round((MAE_rf),3),\n",
        "       'MSE':round((MSE_rf),3),\n",
        "       'RMSE':round((RMSE_rf),3),\n",
        "       'R2_score':round((r2_rf),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_rf ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "N-2lWdBEZziA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity- Residual plot\n",
        "plt.scatter((y_pred_test_r),(y_test)-(y_pred_test_r))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Q_Yj_4XZ1qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.feature_importances_"
      ],
      "metadata": {
        "id": "nl3AemTUZ5HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FEATURES STORED**"
      ],
      "metadata": {
        "id": "bkFPaqBUaCqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = rf_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "rWFyYo6yaJid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "DghPEMlMaMa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "tMyJt4S0aOnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "rf_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "DF80suAMaTxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = rf_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "pY7yUh1taXgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lNkMhfd6abR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRADIENT BOOSTING**"
      ],
      "metadata": {
        "id": "_cK7bQtHahse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "\n",
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "DZ1BhsDoakSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_g = gb_model.predict(X_train)\n",
        "y_pred_test_g = gb_model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "inflK7cJanYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",gb_model.score(X_train,y_train))\n",
        "#calculate MSE\n",
        "MSE_gb= mean_squared_error(y_train, y_pred_train_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_gb=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_gb= mean_absolute_error(y_train, y_pred_train_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_gb= r2_score(y_train, y_pred_train_g)\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score(y_train, y_pred_train_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "4txf3_E1arwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our train set's r2 score value is 0.87 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "7wFi84f7axo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Gradient boosting regression ',\n",
        "       'MAE':round((MAE_gb),3),\n",
        "       'MSE':round((MSE_gb),3),\n",
        "       'RMSE':round((RMSE_gb),3),\n",
        "       'R2_score':round((r2_gb),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
        "       }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "f1lHcsQkcNY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the packages\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#calculate MSE\n",
        "MSE_gb= mean_squared_error(y_test, y_pred_test_g)\n",
        "print(\"MSE :\",MSE_gb)\n",
        "\n",
        "#calculate RMSE\n",
        "RMSE_gb=np.sqrt(MSE_gb)\n",
        "print(\"RMSE :\",RMSE_gb)\n",
        "\n",
        "\n",
        "#calculate MAE\n",
        "MAE_gb= mean_absolute_error(y_test, y_pred_test_g)\n",
        "print(\"MAE :\",MAE_gb)\n",
        "\n",
        "\n",
        "#import the packages\n",
        "from sklearn.metrics import r2_score\n",
        "#calculate r2 and adjusted r2\n",
        "r2_gb= r2_score((y_test), (y_pred_test_g))\n",
        "print(\"R2 :\",r2_gb)\n",
        "Adjusted_R2_gb = (1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "pzp1Cg4McQ9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The r2_score for the test set is 0.86. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**"
      ],
      "metadata": {
        "id": "OCc99gCzcT-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Gradient boosting regression ',\n",
        "       'MAE':round((MAE_gb),3),\n",
        "       'MSE':round((MSE_gb),3),\n",
        "       'RMSE':round((RMSE_gb),3),\n",
        "       'R2_score':round((r2_gb),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gb ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "foarGdBTcYSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_g),(y_test)-(y_pred_test_g))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9PVmA5xXcbag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.feature_importances_"
      ],
      "metadata": {
        "id": "J_Q6ReTGcfnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FEATURES STORED**"
      ],
      "metadata": {
        "id": "Dd5tRChXciPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = gb_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "Du9NAsQdcnqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "ddYPtMFBcqM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.head()"
      ],
      "metadata": {
        "id": "vdxRkXKccsJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "X8GA4JX0cvCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "E7lgYs7QczwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = gb_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "zi95_BDdc2A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iocNZlOQc4m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hyperparameter tuning**"
      ],
      "metadata": {
        "id": "-bKw6NKidA-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before proceding to try next models, let us try to tune some hyperparameters and see if the performance of our model improves.\n",
        "\n",
        "Hyperparameter tuning is the process of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a model argument whose value is set before the learning process begins. The key to machine learning algorithms is hyperparameter tuning."
      ],
      "metadata": {
        "id": "-GJaNOZ6dGQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> Using GridSearchCV"
      ],
      "metadata": {
        "id": "OgF0L3RMdJrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV helps to loop through predefined hyperparameters and fit the model on the training set. So, in the end, we can select the best parameters from the listed hyperparameters."
      ],
      "metadata": {
        "id": "UNtS5F0YdN2A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1s-aDeIKgh5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Gradient Boosting Regressor with GridSearchCV**"
      ],
      "metadata": {
        "id": "jF9BJ3b_gc-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Provide the range of values for chosen hyperparameters**"
      ],
      "metadata": {
        "id": "HDEWKfQCglYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Grid\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ],
      "metadata": {
        "id": "ERDzTJLpgoSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_dict"
      ],
      "metadata": {
        "id": "0HXNLFjqgoIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing Gradient Boosting Regressor**"
      ],
      "metadata": {
        "id": "3e7D72xCgxkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Create an instance of the GradientBoostingRegressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "# Grid search\n",
        "param_dict = {'learning_rate': [0.1, 0.01],\n",
        "              'n_estimators': [50, 100],\n",
        "              'max_depth': [3, 5]}\n",
        "\n",
        "gb_grid = GridSearchCV(estimator=gb_model,\n",
        "                       param_grid=param_dict,\n",
        "                       cv=3, verbose=2, n_jobs=-1)\n",
        "\n",
        "gb_grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "grb4cp2mg16b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "mwr-XmBfg5w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model = gb_grid.best_estimator_"
      ],
      "metadata": {
        "id": "ogqI0Yd3hA5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_grid.best_params_"
      ],
      "metadata": {
        "id": "522ZSZ7rhLLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on train and test data\n",
        "\n",
        "y_pred_train_g_g = gb_optimal_model.predict(X_train)\n",
        "y_pred_g_g= gb_optimal_model.predict(X_test)"
      ],
      "metadata": {
        "id": "hJikWlzBhT5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Model Score:\",gb_optimal_model.score(X_train,y_train))\n",
        "MSE_gbh= mean_squared_error(y_train, y_pred_train_g_g)\n",
        "print(\"MSE :\",MSE_gbh)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gbh)\n",
        "print(\"RMSE :\",RMSE_gbh)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_train, y_pred_train_g_g)\n",
        "print(\"MAE :\",MAE_gbh)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score(y_train, y_pred_train_g_g)\n",
        "print(\"R2 :\",r2_gbh)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score(y_train, y_pred_train_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "aemZrl-vhOh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Looks like our train set's r2 score value is 0.94 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ],
      "metadata": {
        "id": "QLfmWqOBhZaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Gradient Boosting gridsearchcv ',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((r2_gbh),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
        "      }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "IJs5b1w7hbsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "MSE_gbh= mean_squared_error(y_test, y_pred_g_g)\n",
        "print(\"MSE :\",MSE_gbh)\n",
        "\n",
        "RMSE_gbh=np.sqrt(MSE_gbh)\n",
        "print(\"RMSE :\",RMSE_gbh)\n",
        "\n",
        "\n",
        "MAE_gbh= mean_absolute_error(y_test, y_pred_g_g)\n",
        "print(\"MAE :\",MAE_gbh)\n",
        "\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_gbh= r2_score((y_test), (y_pred_g_g))\n",
        "print(\"R2 :\",r2_gbh)\n",
        "Adjusted_R2_gbh = (1-(1-r2_score(y_test, y_pred_g_g))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_g_g)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "0o3W7pL2hfEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tunning certainly showed a better result, r2 was 0.91 on test and mae and rmse was lowered. Overall model show good result."
      ],
      "metadata": {
        "id": "hn1tC2V2hisp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Gradient Boosting gridsearchcv ',\n",
        "       'MAE':round((MAE_gbh),3),\n",
        "       'MSE':round((MSE_gbh),3),\n",
        "       'RMSE':round((RMSE_gbh),3),\n",
        "       'R2_score':round((r2_gbh),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_gbh ),2)\n",
        "      }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "mv4-l7_ahiSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_g_g),(y_test)-(y_pred_g_g))\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ad-2AQ4hmif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_optimal_model.feature_importances_"
      ],
      "metadata": {
        "id": "IyGlYH5zhp9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FEATURES STORED"
      ],
      "metadata": {
        "id": "-A_0En2Yhyuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = gb_optimal_model.feature_importances_\n",
        "\n",
        "importance_dict = {'Feature' : list(X_train.columns),\n",
        "                   'Feature Importance' : importances}\n",
        "\n",
        "importance_df = pd.DataFrame(importance_dict)"
      ],
      "metadata": {
        "id": "-hCcXK4fh05a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)"
      ],
      "metadata": {
        "id": "7a2v8fEPhyMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.head()"
      ],
      "metadata": {
        "id": "aki0utMph4zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importance_df.sort_values(by=['Feature Importance'],ascending=False)"
      ],
      "metadata": {
        "id": "89_vz-_2h7C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "FwYG1RNph_68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = gb_model.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "70vWb2priC0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(10,20))\n",
        "plt.title('Feature Importance')\n",
        "plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J65p28muiFTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During our analysis, we conducted an initial exploratory data analysis (EDA) on all the features in our dataset. Firstly, we analysed our dependent variable 'Rented Bike count' and applied transformations as necessar. We then examined the categorical variables and removed those with majority of one class. We also studied the numerical variables, calculated their correlations, distribution and the their relationships with the dependent variable. Additionally we removed some numerical features that contained mostly 0 values and applied one-hot encoding to the categorical variables.\\\n",
        "Subsequently, we employed 7 machine learning algorithms including Linear Regression,Lasso , Ridge, Elastic Net, Decision Tree, Random Forest and Gradient Booster. We also performed hyperparameter tuning to enhance the performance of our models. The evaluation of our models resulted in the following findings :"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying the results of evaluation metric values for all models\n",
        "result=pd.concat([training_df,test_df],keys=['Training set','Test set'])\n",
        "result"
      ],
      "metadata": {
        "id": "elXsfUGsidyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train a model to predict the number of rented bike count in given weather conditions. First, we do Exploratory Data Analysis on the data set. We look for null values that is not found in dataset and outliers and appropriately modify them. We also perform correlation analysis to extract out the important and relevant feature set and later perform feature engineering.\n",
        "\n",
        "* Gradient Boosting gridsearchcv model shows promising result with R2 score of 0.91, therefore it can be used to solve this problem.\n",
        "* Temperatue, Functioning_Day_Yes, Humidity, Rainfall and  Solar radiation are major driving factors for the Bike rent demand.\n",
        "* Bike demand shows peek around 8-9 AM in the morning and 6 - 7pm in the evening.\n",
        "* People prefer to rent bike more in summer than in winter.\n",
        "* Bike demand is more on clear days than on snowy or rainy days.\n",
        "* Temperature range from 22 to 25(°C) has more demand for bike."
      ],
      "metadata": {
        "id": "-WDuX9VmicYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the current analysis may be insightful, it is important to note that the dataset is time-dependent and variables such as temperature, windspeed and solar radiation may not always remain consistent. As a result there may be situations where the model fails to perform well. As field of machine learning is constantly evolving, it is necessary to stay up-to-date with the latest developments and be prepared to handle unexpected scenarios. Maintaining a strong understanding of Machine Learning concepts will undoubtely provide an advantage in staying ahead in the future."
      ],
      "metadata": {
        "id": "JBfBKitrivxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}